{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sargonxg/TACITUS-Knowledge-Pipeline/blob/main/TACITUS_PIPELINE_NoOutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X9wASb6xhD2"
      },
      "source": [
        "# üèõÔ∏è TACITUS Knowledge Pipeline v7.0\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Production-grade pipeline for building *auditable* conflict intelligence graphs (Grounding ‚Üí Context ‚Üí Reasoning).**\n",
        "\n",
        "---\n",
        "\n",
        "## üó∫Ô∏è QUICK START GUIDE\n",
        "\n",
        "### Phase 1: Setup (Run Once)\n",
        "| Step | Block | What It Does | Time |\n",
        "|------|-------|--------------|------|\n",
        "| 1 | **BLOCK 1** | Install dependencies & configure | ~30‚Äì60s |\n",
        "| 2 | **BLOCK 2** | Define core data schemas (Ontology) | Instant |\n",
        "| 3 | **BLOCK 2A‚Äì2C** | Evidence/provenance + layer helpers + constraints | ~5‚Äì15s |\n",
        "| 4 | **BLOCK 3** | Initialize Vector Engine & OntoRAG | ~5‚Äì15s |\n",
        "| 5 | **BLOCK 4‚Äì4A** | Test connections + apply DB constraints/indexes | ~5‚Äì15s |\n",
        "\n",
        "### Phase 2: Build Your Knowledge Base\n",
        "| Step | Block | What It Does | Time |\n",
        "|------|-------|--------------|------|\n",
        "| 6 | **BLOCK 5** | Seed conflict theories (GND graph) | ~60‚Äì120s |\n",
        "| 7 | **BLOCK 6 + 6A** | Upload/process docs + deterministic cleaning | Varies |\n",
        "| 8 | **BLOCK 6B** | HITL dedup suggestions (Actors) | Varies |\n",
        "| 9 | **BLOCK 6C‚Äì6D** | Evidence-first ingest + QA & run metrics | Varies |\n",
        "| 10 | **BLOCK 6E** | Build Reasoning Graph (issues, contradictions, leverage scores) | ~10‚Äì60s |\n",
        "\n",
        "### Phase 3: Query & Visualize\n",
        "| Step | Block | What It Does | Time |\n",
        "|------|-------|--------------|------|\n",
        "| 11 | **BLOCK 7** | Semantic search (natural language) | Instant |\n",
        "| 12 | **BLOCK 8** | Cypher Query Lab | Instant |\n",
        "| 13 | **BLOCK 9** | Visualization Dashboard | Instant |\n",
        "\n",
        "### Phase 4: Exports & Integrations\n",
        "| Step | Block | What It Does | Time |\n",
        "|------|-------|--------------|------|\n",
        "| 14 | **BLOCK 10‚Äì10A** | Export + bundle audit artifacts | Instant |\n",
        "| 15 | **BLOCK 11** | Transfer a case from Neo4j ‚Üí FalkorDB | Varies |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahLM3aXGxhEC"
      },
      "source": [
        "---\n",
        "# üì¶ BLOCK 1: Installation & Configuration\n",
        "**Run this first. Installs all dependencies and sets up connections.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmNw-mP8xhEF"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 1: INSTALLATION & CONFIGURATION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Install all dependencies and configure the environment\n",
        "# ‚è±Ô∏è TIME: ~30 seconds\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install --quiet \\\n",
        "    neo4j \\\n",
        "    google-cloud-aiplatform==1.49.0 \\\n",
        "    \"pydantic>=2.0\" \\\n",
        "    tenacity \\\n",
        "    rapidfuzz \\\n",
        "    pyvis \\\n",
        "    plotly \\\n",
        "    pandas \\\n",
        "    numpy \\\n",
        "    tqdm \\\n",
        "    rich \\\n",
        "    nest_asyncio \\\n",
        "    faiss-cpu \\\n",
        "    python-dateutil \\\n",
        "    ipywidgets \\\n",
        "    scikit-learn \\\n",
        "    networkx \\\n",
        "    falkordb \\\n",
        "    redis\n",
        "\n",
        "print(\"‚úÖ Dependencies installed\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# IMPORTS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "import uuid\n",
        "import asyncio\n",
        "import warnings\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dateutil import parser as date_parser\n",
        "\n",
        "# Google & Vertex AI\n",
        "from google.colab import auth, files, userdata\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
        "\n",
        "# Graph & Data\n",
        "from neo4j import GraphDatabase\n",
        "from pydantic import BaseModel, Field\n",
        "from rapidfuzz import fuzz\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# Vector Search\n",
        "import faiss\n",
        "\n",
        "# UI & Progress\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "# These libraries are used in later blocks and were part of the erroneous import chain\n",
        "import sklearn\n",
        "import networkx as nx\n",
        "\n",
        "# Async support in Colab\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"vertexai\")\n",
        "\n",
        "console = Console()\n",
        "print(\"‚úÖ Imports complete\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CONFIGURATION\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration for TACITUS v6.0\"\"\"\n",
        "\n",
        "    # Google Cloud\n",
        "    project_id: str = 'tacitus-63ba5'\n",
        "    location: str = 'us-central1'\n",
        "\n",
        "    # Neo4j Connection (Aura)\n",
        "    neo4j_uri: str = 'neo4j+s://84c54653.databases.neo4j.io'\n",
        "    neo4j_user: str = 'neo4j'\n",
        "    neo4j_password: str = 'kPgV2l44VSpSyu6hBNI4R5lgKZG-S2jpefHu0k5ivNo'\n",
        "\n",
        "    # Models\n",
        "    extraction_model: str = 'gemini-2.5-flash-lite'  # Fast LLM for extraction\n",
        "    embedding_model: str = 'text-embedding-005'      # Embedding model\n",
        "    embedding_dimensions: int = 768                   # Vector dimensions (768 is efficient)\n",
        "\n",
        "    # Processing\n",
        "    chunk_size: int = 20000      # Characters per chunk\n",
        "    chunk_overlap: int = 1000    # Overlap between chunks\n",
        "    er_threshold: float = 85.0   # Entity resolution similarity threshold\n",
        "\n",
        "    # Defaults\n",
        "    default_date: str = \"2024-01-01\"\n",
        "\n",
        "    # Runtime\n",
        "    run_id: str = field(default_factory=lambda: f\"RUN_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    dry_run: bool = False\n",
        "\n",
        "config = Config()\n",
        "print(f\"‚úÖ Configuration loaded (Run ID: {config.run_id})\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AUTHENTICATE & CONNECT\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüîê Authenticating with Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "vertexai.init(project=config.project_id, location=config.location)\n",
        "print(f\"‚úÖ Vertex AI initialized: {config.project_id}\")\n",
        "\n",
        "print(\"\\nüîó Connecting to Neo4j...\")\n",
        "try:\n",
        "    driver = GraphDatabase.driver(\n",
        "        config.neo4j_uri,\n",
        "        auth=(config.neo4j_user, config.neo4j_password)\n",
        "    )\n",
        "    driver.verify_connectivity()\n",
        "    print(\"‚úÖ Neo4j connection verified\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Neo4j connection failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Create indexes for performance\n",
        "print(\"\\nüìÑ Creating indexes...\")\n",
        "with driver.session() as session:\n",
        "    indexes = [\n",
        "        'CREATE INDEX IF NOT EXISTS FOR (a:Actor) ON (a.case_id)',\n",
        "        'CREATE INDEX IF NOT EXISTS FOR (a:Actor) ON (a.entity_id)',\n",
        "        'CREATE INDEX IF NOT EXISTS FOR (c:Case) ON (c.case_id)',\n",
        "        'CREATE INDEX IF NOT EXISTS FOR (e:Event) ON (e.case_id)',\n",
        "        'CREATE INDEX IF NOT EXISTS FOR (t:Theory) ON (t.entity_id)',\n",
        "        'CREATE INDEX IF NOT EXISTS FOR (n) ON (n.graph_category)'\n",
        "    ]\n",
        "    for idx in indexes:\n",
        "        try:\n",
        "            session.run(idx)\n",
        "        except Exception:\n",
        "            pass  # Index may already exist\n",
        "print(\"‚úÖ Indexes ready\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ BLOCK 1 COMPLETE - Environment Ready!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNUPaHUhxhEL"
      },
      "source": [
        "---\n",
        "# üìê BLOCK 2: Data Ontology (Schemas)\n",
        "**Defines the structure of all data we extract and store.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wsQMT0dxhEM"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 2: DATA ONTOLOGY (PYDANTIC SCHEMAS)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Define the structure of all extracted data\n",
        "# ‚è±Ô∏è TIME: Instant\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CTX (CASE CONTEXT) SCHEMAS - Specific case data\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "class ExtractedActor(BaseModel):\n",
        "    \"\"\"A person, organization, or entity involved in the conflict.\"\"\"\n",
        "    name: str\n",
        "    actor_type: str = \"unknown\"  # person, org, state, group\n",
        "    role: Optional[str] = None\n",
        "    description: Optional[str] = None\n",
        "    is_primary: bool = False\n",
        "    aliases: List[str] = Field(default_factory=list)\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ActorRelationship(BaseModel):\n",
        "    \"\"\"A relationship between two actors.\"\"\"\n",
        "    source: str\n",
        "    target: str\n",
        "    rel_type: str  # OPPOSES, ALLIES_WITH, INFLUENCES, DEPENDS_ON\n",
        "    confidence: float = 0.8\n",
        "    valid_from: Optional[str] = None\n",
        "    valid_until: Optional[str] = None\n",
        "    status: str = \"active\"\n",
        "\n",
        "class ExtractedEvent(BaseModel):\n",
        "    \"\"\"A significant event in the conflict timeline.\"\"\"\n",
        "    description: str\n",
        "    event_type: str = \"action\"  # action, escalation, negotiation, agreement\n",
        "    date: Optional[str] = None\n",
        "    actor_names: List[str] = Field(default_factory=list)\n",
        "    impact_level: str = \"moderate\"  # critical, high, moderate, low\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ExtractedClaim(BaseModel):\n",
        "    \"\"\"A claim, demand, or statement made by an actor.\"\"\"\n",
        "    statement: str\n",
        "    actor_name: str\n",
        "    claim_type: str = \"assertion\"  # demand, threat, accusation, promise\n",
        "    target_actor: Optional[str] = None\n",
        "    valid_from: Optional[str] = None\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ExtractedInterest(BaseModel):\n",
        "    \"\"\"An interest or goal of an actor.\"\"\"\n",
        "    description: str\n",
        "    actor_name: str\n",
        "    interest_type: str = \"substantive\"  # substantive, procedural, psychological\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ExtractedLeverage(BaseModel):\n",
        "    \"\"\"Leverage or power that an actor holds.\"\"\"\n",
        "    description: str\n",
        "    actor_name: str\n",
        "    leverage_type: str = \"structural\"  # coercive, economic, informational, structural\n",
        "    target_actors: List[str] = Field(default_factory=list)\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ExtractedConstraint(BaseModel):\n",
        "    \"\"\"A constraint limiting an actor's options.\"\"\"\n",
        "    description: str\n",
        "    actor_name: Optional[str] = None\n",
        "    constraint_type: str = \"political\"  # political, legal, economic, resource\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ExtractedCommitment(BaseModel):\n",
        "    \"\"\"A commitment or agreement between parties.\"\"\"\n",
        "    description: str\n",
        "    parties: List[str] = Field(default_factory=list)\n",
        "    status: str = \"active\"  # active, broken, fulfilled\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class ExtractedNarrative(BaseModel):\n",
        "    \"\"\"A narrative frame promoted by an actor.\"\"\"\n",
        "    description: str\n",
        "    actor_name: str\n",
        "    frame_type: str = \"neutral\"  # victim, aggressor, mediator, defender\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class FullExtraction(BaseModel):\n",
        "    \"\"\"Container for all extracted data from a chunk.\"\"\"\n",
        "    actors: List[ExtractedActor] = Field(default_factory=list)\n",
        "    relationships: List[ActorRelationship] = Field(default_factory=list)\n",
        "    events: List[ExtractedEvent] = Field(default_factory=list)\n",
        "    claims: List[ExtractedClaim] = Field(default_factory=list)\n",
        "    interests: List[ExtractedInterest] = Field(default_factory=list)\n",
        "    leverage: List[ExtractedLeverage] = Field(default_factory=list)\n",
        "    constraints: List[ExtractedConstraint] = Field(default_factory=list)\n",
        "    commitments: List[ExtractedCommitment] = Field(default_factory=list)\n",
        "    narratives: List[ExtractedNarrative] = Field(default_factory=list)\n",
        "\n",
        "    def stats(self) -> Dict[str, int]:\n",
        "        \"\"\"Return counts for each entity type.\"\"\"\n",
        "        return {k: len(getattr(self, k)) for k in self.model_fields.keys()}\n",
        "\n",
        "    def total_entities(self) -> int:\n",
        "        \"\"\"Total number of extracted entities.\"\"\"\n",
        "        return sum(self.stats().values())\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# GND (GROUNDING) SCHEMAS - Abstract theories\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "class GNDConcept(BaseModel):\n",
        "    \"\"\"An abstract concept in conflict theory.\"\"\"\n",
        "    name: str\n",
        "    definition: str\n",
        "    concept_type: str = \"phenomenon\"\n",
        "    indicators: List[str] = Field(default_factory=list)\n",
        "\n",
        "class GNDTheory(BaseModel):\n",
        "    \"\"\"A named conflict theory or framework.\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    proponent: Optional[str] = \"Academic\"\n",
        "    core_concepts: List[str] = Field(default_factory=list)\n",
        "\n",
        "class GNDCausalLink(BaseModel):\n",
        "    \"\"\"A causal relationship between concepts.\"\"\"\n",
        "    source_concept: str\n",
        "    target_concept: str\n",
        "    relation_type: str  # causes, enables, prevents, correlates\n",
        "    justification: str\n",
        "    confidence: float = 0.8\n",
        "\n",
        "class GNDPayload(BaseModel):\n",
        "    \"\"\"Container for grounding graph data.\"\"\"\n",
        "    concepts: List[GNDConcept] = Field(default_factory=list)\n",
        "    theories: List[GNDTheory] = Field(default_factory=list)\n",
        "    causal_links: List[GNDCausalLink] = Field(default_factory=list)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# DOCUMENT STRUCTURES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    \"\"\"A text chunk for processing.\"\"\"\n",
        "    chunk_id: str\n",
        "    content: str\n",
        "    index: int\n",
        "    char_start: int\n",
        "    char_end: int\n",
        "\n",
        "@dataclass\n",
        "class CaseDocument:\n",
        "    \"\"\"A processed case document.\"\"\"\n",
        "    case_id: str\n",
        "    case_name: str\n",
        "    filename: str\n",
        "    full_text: str\n",
        "    sha256: str\n",
        "    chunks: List[Chunk]\n",
        "\n",
        "print(\"‚úÖ Ontology defined:\")\n",
        "print(\"   ‚Ä¢ CTX Schemas: Actor, Event, Claim, Interest, Leverage, Constraint, Commitment, Narrative\")\n",
        "print(\"   ‚Ä¢ GND Schemas: Concept, Theory, CausalLink\")\n",
        "print(\"   ‚Ä¢ Document: Chunk, CaseDocument\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps7jJKypeXpL"
      },
      "source": [
        "---\n",
        "# üßæ BLOCK 2A: Evidence & Provenance Layer (Span-level auditability)\n",
        "**Goal:** Make every extracted object *traceable back to the exact text span(s)* that support it.\n",
        "\n",
        "This extends the pipeline with:\n",
        "- `SourceDoc` ‚Üí `SourceChunk` ‚Üí `TextSpan` nodes\n",
        "- `EVIDENCE_FOR` edges from spans to Actors/Claims/Events/etc.\n",
        "- `ExtractionRun` nodes for run-level metrics, configs, and QA signals\n",
        "\n",
        "> This is the ‚Äúaudit substrate‚Äù: you can always answer *‚Äúwhy does the graph believe X?‚Äù*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ40hmHAdXcW"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 2A: EVIDENCE & PROVENANCE SCHEMAS + HELPERS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "from typing import Iterable\n",
        "import math\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# IDs (stable, deterministic where possible)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def mk_id(prefix: str, *parts: str, n: int = 12) -> str:\n",
        "    payload = \"|\".join([p or \"\" for p in parts]).encode(\"utf-8\")\n",
        "    h = hashlib.sha256(payload).hexdigest()[:n]\n",
        "    return f\"{prefix}{h}\"\n",
        "\n",
        "def doc_id_from_sha(sha256: str) -> str:\n",
        "    return f\"EVD:DOC_{sha256[:12]}\"\n",
        "\n",
        "def span_id(chunk_id: str, start: int, end: int) -> str:\n",
        "    return mk_id(\"EVD:SPN_\", chunk_id, str(start), str(end), n=16)\n",
        "\n",
        "def run_id() -> str:\n",
        "    return f\"RUN_{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}_{uuid.uuid4().hex[:8]}\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Span extraction (cheap, deterministic)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def _find_all_occurrences(haystack: str, needle: str, max_hits: int = 3) -> List[Tuple[int,int]]:\n",
        "    \"\"\"Case-insensitive substring matches; returns [(start,end), ...].\"\"\"\n",
        "    if not needle:\n",
        "        return []\n",
        "    hs = haystack.lower()\n",
        "    nd = needle.lower().strip()\n",
        "    out = []\n",
        "    start = 0\n",
        "    while len(out) < max_hits:\n",
        "        i = hs.find(nd, start)\n",
        "        if i == -1:\n",
        "            break\n",
        "        out.append((i, i + len(nd)))\n",
        "        start = i + len(nd)\n",
        "    return out\n",
        "\n",
        "def _excerpt(text: str, start: int, end: int, window: int = 140) -> str:\n",
        "    a = max(0, start - window)\n",
        "    b = min(len(text), end + window)\n",
        "    snippet = text[a:b].replace(\"\\n\", \" \").strip()\n",
        "    return snippet\n",
        "\n",
        "def propose_spans_for_chunk(chunk_text: str, actors: List[\"ExtractedActor\"], claims: List[\"ExtractedClaim\"]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Produce lightweight candidate spans for a chunk.\n",
        "    - Actors: match on canonical actor name (and aliases)\n",
        "    - Claims: attempt exact match on a short prefix (first ~12 words)\n",
        "    \"\"\"\n",
        "    spans = []\n",
        "\n",
        "    # Actors\n",
        "    for a in actors:\n",
        "        needles = [a.name] + list(getattr(a, \"aliases\", []) or [])\n",
        "        needles = [n for n in needles if n and len(n) >= 3]\n",
        "        seen = set()\n",
        "        for nd in needles:\n",
        "            for (s,e) in _find_all_occurrences(chunk_text, nd, max_hits=2):\n",
        "                key = (s,e, a.name)\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                spans.append({\n",
        "                    \"entity_kind\": \"Actor\",\n",
        "                    \"entity_key\": a.name,\n",
        "                    \"start_in_chunk\": s,\n",
        "                    \"end_in_chunk\": e,\n",
        "                    \"method\": \"string_match\",\n",
        "                    \"confidence\": 0.85 if nd != a.name else 0.95,\n",
        "                    \"excerpt\": _excerpt(chunk_text, s, e)\n",
        "                })\n",
        "\n",
        "    # Claims (best-effort)\n",
        "    for c in claims:\n",
        "        stmt = (c.statement or \"\").strip()\n",
        "        if len(stmt) < 12:\n",
        "            continue\n",
        "        short = \" \".join(stmt.split()[:12])\n",
        "        for (s,e) in _find_all_occurrences(chunk_text, short, max_hits=1):\n",
        "            spans.append({\n",
        "                \"entity_kind\": \"Claim\",\n",
        "                \"entity_key\": stmt,\n",
        "                \"start_in_chunk\": s,\n",
        "                \"end_in_chunk\": e,\n",
        "                \"method\": \"claim_prefix_match\",\n",
        "                \"confidence\": 0.75,\n",
        "                \"excerpt\": _excerpt(chunk_text, s, e)\n",
        "            })\n",
        "\n",
        "    return spans\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Neo4j write helpers (Evidence nodes)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def write_source_doc_and_chunks(driver, doc: \"CaseDocument\") -> None:\n",
        "    \"\"\"\n",
        "    Creates:\n",
        "      (Case)-[:HAS_SOURCE]->(SourceDoc)-[:HAS_CHUNK]->(SourceChunk)\n",
        "    and stores cleaning + lint metadata if present on doc.\n",
        "    \"\"\"\n",
        "    docid = doc_id_from_sha(doc.sha256)\n",
        "    meta = {\n",
        "        \"filename\": doc.filename,\n",
        "        \"sha256\": doc.sha256,\n",
        "        \"case_id\": doc.case_id,\n",
        "        \"case_name\": doc.case_name,\n",
        "        \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"graph_category\": \"EVD\",\n",
        "        \"cleaning_audit\": json.dumps([h.__dict__ for h in getattr(doc, \"cleaning_audit\", [])], default=str),\n",
        "        \"lint_flags\": json.dumps(getattr(doc, \"lint_flags\", {}), default=str),\n",
        "    }\n",
        "\n",
        "    with driver.session() as s:\n",
        "        s.run(\"\"\"\n",
        "            MERGE (c:Case {case_id: $case_id})\n",
        "            SET c.case_name = $case_name,\n",
        "                c.graph_category = COALESCE(c.graph_category,'OPS'),\n",
        "                c.created_at = COALESCE(c.created_at, $created_at)\n",
        "            MERGE (d:SourceDoc {doc_id: $doc_id})\n",
        "            SET d += $meta\n",
        "            MERGE (c)-[:HAS_SOURCE]->(d)\n",
        "        \"\"\", case_id=doc.case_id, case_name=doc.case_name, created_at=meta[\"created_at\"], doc_id=docid, meta=meta)\n",
        "\n",
        "        # Chunks\n",
        "        for ch in doc.chunks:\n",
        "            s.run(\"\"\"\n",
        "                MERGE (k:SourceChunk {chunk_id: $chunk_id})\n",
        "                SET k.case_id=$case_id,\n",
        "                    k.doc_id=$doc_id,\n",
        "                    k.index=$idx,\n",
        "                    k.char_start=$cs,\n",
        "                    k.char_end=$ce,\n",
        "                    k.graph_category='EVD'\n",
        "                MERGE (d:SourceDoc {doc_id: $doc_id})\n",
        "                MERGE (d)-[:HAS_CHUNK]->(k)\n",
        "            \"\"\", chunk_id=ch.chunk_id, case_id=doc.case_id, doc_id=docid, idx=ch.index, cs=ch.char_start, ce=ch.char_end)\n",
        "\n",
        "def write_extraction_run(driver, rid: str, case_id: str, config_snapshot: Dict[str, Any]) -> None:\n",
        "    with driver.session() as s:\n",
        "        s.run(\"\"\"\n",
        "            MERGE (r:ExtractionRun {run_id: $rid})\n",
        "            SET r.case_id=$cid,\n",
        "                r.started_at=$ts,\n",
        "                r.config=$cfg,\n",
        "                r.graph_category='OPS'\n",
        "            MERGE (c:Case {case_id: $cid})\n",
        "            MERGE (c)-[:HAS_RUN]->(r)\n",
        "        \"\"\", rid=rid, cid=case_id, ts=datetime.now(timezone.utc).isoformat(), cfg=json.dumps(config_snapshot, default=str))\n",
        "\n",
        "def attach_run_source(driver, rid: str, doc: \"CaseDocument\") -> None:\n",
        "    with driver.session() as s:\n",
        "        s.run(\"\"\"\n",
        "            MATCH (r:ExtractionRun {run_id:$rid})\n",
        "            MATCH (d:SourceDoc {doc_id:$docid})\n",
        "            MERGE (r)-[:USED_SOURCE]->(d)\n",
        "        \"\"\", rid=rid, docid=doc_id_from_sha(doc.sha256))\n",
        "\n",
        "def write_spans_and_evidence(driver, doc: \"CaseDocument\", ext: \"FullExtraction\", rid: str) -> int:\n",
        "    \"\"\"\n",
        "    Creates TextSpan nodes per chunk and links them to extracted entities.\n",
        "    We match entities by (case_id + properties) instead of re-deriving entity_id hashes.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    with driver.session() as s:\n",
        "        for ch in doc.chunks:\n",
        "            spans = propose_spans_for_chunk(ch.content, ext.actors, ext.claims)\n",
        "            for sp in spans:\n",
        "                sid = span_id(ch.chunk_id, sp[\"start_in_chunk\"], sp[\"end_in_chunk\"])\n",
        "                # doc-level coordinates\n",
        "                ds = int(ch.char_start + sp[\"start_in_chunk\"])\n",
        "                de = int(ch.char_start + sp[\"end_in_chunk\"])\n",
        "                s.run(\"\"\"\n",
        "                    MERGE (k:SourceChunk {chunk_id:$chunk_id})\n",
        "                    MERGE (t:TextSpan {span_id:$sid})\n",
        "                    SET t.case_id=$cid,\n",
        "                        t.chunk_id=$chunk_id,\n",
        "                        t.doc_char_start=$ds,\n",
        "                        t.doc_char_end=$de,\n",
        "                        t.excerpt=$ex,\n",
        "                        t.method=$m,\n",
        "                        t.confidence=$conf,\n",
        "                        t.graph_category='EVD'\n",
        "                    MERGE (k)-[:HAS_SPAN]->(t)\n",
        "                    WITH t\n",
        "                    MATCH (r:ExtractionRun {run_id:$rid})\n",
        "                    MERGE (t)-[:FOUND_IN_RUN]->(r)\n",
        "                \"\"\", chunk_id=ch.chunk_id, sid=sid, cid=doc.case_id, ds=ds, de=de,\n",
        "                     ex=sp[\"excerpt\"], m=sp[\"method\"], conf=float(sp[\"confidence\"]), rid=rid)\n",
        "\n",
        "                # Evidence link\n",
        "                if sp[\"entity_kind\"] == \"Actor\":\n",
        "                    s.run(\"\"\"\n",
        "                        MATCH (t:TextSpan {span_id:$sid, case_id:$cid})\n",
        "                        MATCH (a:Actor {name:$name, case_id:$cid})\n",
        "                        MERGE (t)-[:EVIDENCE_FOR]->(a)\n",
        "                    \"\"\", sid=sid, cid=doc.case_id, name=sp[\"entity_key\"])\n",
        "                elif sp[\"entity_kind\"] == \"Claim\":\n",
        "                    s.run(\"\"\"\n",
        "                        MATCH (t:TextSpan {span_id:$sid, case_id:$cid})\n",
        "                        MATCH (c:Claim {statement:$stmt, case_id:$cid})\n",
        "                        MERGE (t)-[:EVIDENCE_FOR]->(c)\n",
        "                    \"\"\", sid=sid, cid=doc.case_id, stmt=sp[\"entity_key\"])\n",
        "\n",
        "                total += 1\n",
        "    return total\n",
        "\n",
        "print(\"‚úÖ Evidence/Provenance helpers ready (SourceDoc/Chunk/Span + ExtractionRun).\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxvZPvj9qEct"
      },
      "source": [
        "---\n",
        "# üß± BLOCK 2B: Layered Graph Conventions (GND / CTX / RZN / EVD)\n",
        "**Goal:** Keep the graph interpretable as it grows.\n",
        "\n",
        "We standardize:\n",
        "- `graph_category` values\n",
        "- helper utilities for ‚Äúlayer-aware‚Äù Cypher filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLuyCd4RMqW5"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 2B: LAYER CONVENTIONS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "GRAPH_LAYERS = {\n",
        "    \"GND\": \"Grounding Graph (theories, concepts, causal links)\",\n",
        "    \"CTX\": \"Case Context Graph (actors, claims, events, leverage...)\",\n",
        "    \"RZN\": \"Reasoning Graph (derived issues, contradictions, diagnostics)\",\n",
        "    \"EVD\": \"Evidence Graph (source docs, chunks, spans, run traces)\",\n",
        "    \"OPS\": \"Operational metadata (runs, QA metrics, pipeline state)\"\n",
        "}\n",
        "\n",
        "def cypher_layer_filter(layer: str = None) -> str:\n",
        "    if not layer:\n",
        "        return \"TRUE\"\n",
        "    return f\"n.graph_category = '{layer}'\"\n",
        "\n",
        "print(\"‚úÖ Layer conventions loaded:\")\n",
        "for k,v in GRAPH_LAYERS.items():\n",
        "    print(f\"   ‚Ä¢ {k}: {v}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK3BpkJHnrvc"
      },
      "source": [
        "---\n",
        "# üß∞ BLOCK 2C: Neo4j Constraints & Indexes (for scale + safety)\n",
        "**Goal:** Prevent accidental duplicates and speed up large-case queries.\n",
        "\n",
        "This block creates:\n",
        "- uniqueness constraints (entity IDs, case IDs, run IDs)\n",
        "- indexes on `case_id`, `name`, and provenance IDs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvyhgJ0ZdkEp"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 2C: NEO4J CONSTRAINTS & INDEXES\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def _ddl(driver, statements: List[str]) -> List[Tuple[str, str]]:\n",
        "    results = []\n",
        "    with driver.session() as s:\n",
        "        for st in statements:\n",
        "            try:\n",
        "                s.run(st)\n",
        "                results.append((st, \"ok\"))\n",
        "            except Exception as e:\n",
        "                results.append((st, f\"skip: {type(e).__name__}: {e}\"))\n",
        "    return results\n",
        "\n",
        "def ensure_neo4j_constraints(driver) -> None:\n",
        "    ddl = []\n",
        "\n",
        "    ddl += [\n",
        "        \"CREATE CONSTRAINT case_id_unique IF NOT EXISTS FOR (c:Case) REQUIRE c.case_id IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT run_id_unique IF NOT EXISTS FOR (r:ExtractionRun) REQUIRE r.run_id IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT doc_id_unique IF NOT EXISTS FOR (d:SourceDoc) REQUIRE d.doc_id IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS FOR (k:SourceChunk) REQUIRE k.chunk_id IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT span_id_unique IF NOT EXISTS FOR (t:TextSpan) REQUIRE t.span_id IS UNIQUE\",\n",
        "    ]\n",
        "\n",
        "    for label in [\"Actor\",\"Claim\",\"Event\",\"Interest\",\"Leverage\",\"Constraint\",\"Commitment\",\"Narrative\",\n",
        "                  \"Concept\",\"Theory\"]:\n",
        "        ddl.append(f\"CREATE CONSTRAINT {label.lower()}_eid_unique IF NOT EXISTS FOR (n:{label}) REQUIRE n.entity_id IS UNIQUE\")\n",
        "\n",
        "    ddl += [\n",
        "        \"CREATE INDEX actor_case_name IF NOT EXISTS FOR (a:Actor) ON (a.case_id, a.name)\",\n",
        "        \"CREATE INDEX claim_case_stmt IF NOT EXISTS FOR (c:Claim) ON (c.case_id, c.statement)\",\n",
        "        \"CREATE INDEX evd_case_doc IF NOT EXISTS FOR (d:SourceDoc) ON (d.case_id, d.doc_id)\",\n",
        "        \"CREATE INDEX run_case IF NOT EXISTS FOR (r:ExtractionRun) ON (r.case_id)\",\n",
        "    ]\n",
        "\n",
        "    out = _ddl(driver, ddl)\n",
        "    ok = sum(1 for _,st in out if st==\"ok\")\n",
        "    print(f\"‚úÖ Neo4j DDL applied: {ok}/{len(out)} statements (others may be skipped depending on Neo4j version).\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Constraints/indexes helper ready. Run Block 4A next (after Block 4 creates `driver`).\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQEB6sgpxhEP"
      },
      "source": [
        "---\n",
        "# üß† BLOCK 3: Vector Engine & OntoRAG\n",
        "**Powers semantic search and intelligent theory selection.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NPMetOsxhEQ"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 3: VECTOR ENGINE & ONTORAG\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Generate embeddings for semantic search and smart extraction\n",
        "# ‚è±Ô∏è TIME: ~5 seconds to initialize\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class VectorEngine:\n",
        "    \"\"\"\n",
        "    Generates vector embeddings using Vertex AI.\n",
        "    These embeddings enable semantic (meaning-based) search.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"text-embedding-005\", dimensions: int = 768):\n",
        "        \"\"\"\n",
        "        Initialize the vector engine.\n",
        "\n",
        "        Args:\n",
        "            model_name: Vertex AI embedding model\n",
        "            dimensions: Output vector dimensions (768 recommended for Neo4j)\n",
        "        \"\"\"\n",
        "        self.model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "        self.dimensions = dimensions\n",
        "        self.model_name = model_name\n",
        "        self._cache = {}  # Cache to avoid redundant API calls\n",
        "        self._cache_hits = 0\n",
        "        self._api_calls = 0\n",
        "\n",
        "    def embed_text(self, text: str, task_type: str = \"RETRIEVAL_DOCUMENT\") -> List[float]:\n",
        "        \"\"\"\n",
        "        Generate embedding for a single text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to embed\n",
        "            task_type: RETRIEVAL_DOCUMENT or RETRIEVAL_QUERY\n",
        "\n",
        "        Returns:\n",
        "            List of floats representing the embedding vector\n",
        "        \"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return [0.0] * self.dimensions\n",
        "\n",
        "        # Check cache\n",
        "        cache_key = f\"{text[:200]}_{task_type}\"\n",
        "        if cache_key in self._cache:\n",
        "            self._cache_hits += 1\n",
        "            return self._cache[cache_key]\n",
        "\n",
        "        # Truncate if too long (model limit ~2048 tokens ‚âà 8000 chars)\n",
        "        text = text[:7500] if len(text) > 7500 else text\n",
        "\n",
        "        try:\n",
        "            self._api_calls += 1\n",
        "            inputs = [TextEmbeddingInput(text, task_type)]\n",
        "            embeddings = self.model.get_embeddings(\n",
        "                inputs,\n",
        "                output_dimensionality=self.dimensions\n",
        "            )\n",
        "            vector = embeddings[0].values\n",
        "            self._cache[cache_key] = vector\n",
        "            return vector\n",
        "        except Exception as e:\n",
        "            console.print(f\"[yellow]‚ö†Ô∏è Embedding failed: {e}[/yellow]\")\n",
        "            return [0.0] * self.dimensions\n",
        "\n",
        "    def embed_batch(self, texts: List[str], task_type: str = \"RETRIEVAL_DOCUMENT\") -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Generate embeddings for multiple texts (batched for efficiency).\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to embed\n",
        "            task_type: RETRIEVAL_DOCUMENT or RETRIEVAL_QUERY\n",
        "\n",
        "        Returns:\n",
        "            List of embedding vectors\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        batch_size = 50  # Vertex AI batch limit\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            batch_clean = [t[:7500] if t else \"empty\" for t in batch]\n",
        "\n",
        "            try:\n",
        "                self._api_calls += 1\n",
        "                inputs = [TextEmbeddingInput(t, task_type) for t in batch_clean]\n",
        "                embeddings = self.model.get_embeddings(\n",
        "                    inputs,\n",
        "                    output_dimensionality=self.dimensions\n",
        "                )\n",
        "                results.extend([e.values for e in embeddings])\n",
        "            except Exception as e:\n",
        "                console.print(f\"[yellow]‚ö†Ô∏è Batch embedding failed: {e}[/yellow]\")\n",
        "                results.extend([[0.0] * self.dimensions] * len(batch))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def stats(self) -> Dict[str, int]:\n",
        "        \"\"\"Return usage statistics.\"\"\"\n",
        "        return {\n",
        "            \"api_calls\": self._api_calls,\n",
        "            \"cache_hits\": self._cache_hits,\n",
        "            \"cache_size\": len(self._cache)\n",
        "        }\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# ONTORAG SELECTOR\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "class OntoRAGSelector:\n",
        "    \"\"\"\n",
        "    OntoRAG: Ontology-based Retrieval Augmented Generation.\n",
        "\n",
        "    Selects only the most relevant theories for each text chunk,\n",
        "    preventing the LLM from getting confused by irrelevant schemas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vector_engine: VectorEngine):\n",
        "        self.vector_engine = vector_engine\n",
        "        self.index = None\n",
        "        self.theories = []  # List of (name, description, entity_id)\n",
        "        self.is_initialized = False\n",
        "\n",
        "    def build_index_from_neo4j(self, driver) -> bool:\n",
        "        \"\"\"\n",
        "        Load all GND theories from Neo4j and build FAISS index.\n",
        "\n",
        "        Returns:\n",
        "            True if index was built successfully\n",
        "        \"\"\"\n",
        "        console.print(\"[cyan]Building OntoRAG index from theories...[/cyan]\")\n",
        "\n",
        "        with driver.session() as s:\n",
        "            result = s.run(\"\"\"\n",
        "                MATCH (t:Theory)\n",
        "                WHERE t.graph_category = 'GND'\n",
        "                RETURN t.name AS name, t.desc AS description, t.entity_id AS eid\n",
        "            \"\"\")\n",
        "            self.theories = [(r['name'], r['description'] or '', r['eid']) for r in result]\n",
        "\n",
        "        if not self.theories:\n",
        "            console.print(\"[yellow]‚ö†Ô∏è No theories found in GND graph. Run Block 5 to seed theories.[/yellow]\")\n",
        "            return False\n",
        "\n",
        "        # Generate embeddings for all theories\n",
        "        theory_texts = [f\"{t[0]}: {t[1]}\" for t in self.theories]\n",
        "        console.print(f\"[cyan]   Embedding {len(theory_texts)} theories...[/cyan]\")\n",
        "        embeddings = self.vector_engine.embed_batch(theory_texts)\n",
        "\n",
        "        # Build FAISS index (Inner Product for cosine similarity)\n",
        "        vectors = np.array(embeddings).astype('float32')\n",
        "        faiss.normalize_L2(vectors)  # Normalize for cosine similarity\n",
        "\n",
        "        self.index = faiss.IndexFlatIP(vectors.shape[1])\n",
        "        self.index.add(vectors)\n",
        "\n",
        "        self.is_initialized = True\n",
        "        console.print(f\"[green]‚úÖ OntoRAG index ready: {len(self.theories)} theories indexed[/green]\")\n",
        "        return True\n",
        "\n",
        "    def select_relevant_theories(self, text: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Find the most relevant theories for a given text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to match against theories\n",
        "            top_k: Number of theories to return\n",
        "\n",
        "        Returns:\n",
        "            List of dicts with theory info and relevance scores\n",
        "        \"\"\"\n",
        "        if not self.is_initialized:\n",
        "            return []\n",
        "\n",
        "        # Embed the query\n",
        "        query_vec = np.array([self.vector_engine.embed_text(text[:3000], \"RETRIEVAL_QUERY\")]).astype('float32')\n",
        "        faiss.normalize_L2(query_vec)\n",
        "\n",
        "        # Search\n",
        "        k = min(top_k, len(self.theories))\n",
        "        scores, indices = self.index.search(query_vec, k)\n",
        "\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx >= 0 and idx < len(self.theories):\n",
        "                name, desc, eid = self.theories[idx]\n",
        "                results.append({\n",
        "                    'name': name,\n",
        "                    'description': desc,\n",
        "                    'entity_id': eid,\n",
        "                    'relevance_score': float(score)\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INITIALIZE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"üß† Initializing Vector Engine...\")\n",
        "vector_engine = VectorEngine(\n",
        "    model_name=config.embedding_model,\n",
        "    dimensions=config.embedding_dimensions\n",
        ")\n",
        "print(f\"‚úÖ Vector Engine ready ({config.embedding_model}, {config.embedding_dimensions}D)\")\n",
        "\n",
        "onto_rag = OntoRAGSelector(vector_engine)\n",
        "print(\"‚úÖ OntoRAG Selector ready (will initialize after seeding theories)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuWtcHdqxhEU"
      },
      "source": [
        "---\n",
        "# üß™ BLOCK 4: Connection Tests\n",
        "**Verify everything is working before proceeding.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIN_VIVnxhEW"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 4: CONNECTION & FUNCTIONALITY TESTS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Verify all components are working correctly\n",
        "# ‚è±Ô∏è TIME: ~10 seconds\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def run_diagnostics():\n",
        "    \"\"\"Run comprehensive diagnostics on all system components.\"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # TEST 1: Neo4j Connection\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\nüîç Test 1: Neo4j Connection\")\n",
        "    try:\n",
        "        with driver.session() as s:\n",
        "            result = s.run(\"RETURN 1 AS test\").single()\n",
        "            assert result['test'] == 1\n",
        "\n",
        "            # Get DB stats\n",
        "            stats = s.run(\"\"\"\n",
        "                MATCH (n)\n",
        "                RETURN count(n) AS nodes,\n",
        "                       count(DISTINCT labels(n)) AS label_types\n",
        "            \"\"\").single()\n",
        "\n",
        "        print(f\"   ‚úÖ Connected - {stats['nodes']} nodes, {stats['label_types']} label types\")\n",
        "        results.append((\"Neo4j\", \"PASS\", f\"{stats['nodes']} nodes\"))\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed: {e}\")\n",
        "        results.append((\"Neo4j\", \"FAIL\", str(e)))\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # TEST 2: Vertex AI LLM\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\nüîç Test 2: Vertex AI LLM\")\n",
        "    try:\n",
        "        model = GenerativeModel(config.extraction_model)\n",
        "        response = model.generate_content(\n",
        "            \"Say 'TACITUS READY' and nothing else.\",\n",
        "            generation_config=GenerationConfig(temperature=0.0, max_output_tokens=20)\n",
        "        )\n",
        "        output = response.text.strip()\n",
        "        print(f\"   ‚úÖ LLM responded: '{output}'\")\n",
        "        results.append((\"LLM\", \"PASS\", config.extraction_model))\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed: {e}\")\n",
        "        results.append((\"LLM\", \"FAIL\", str(e)))\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # TEST 3: Vector Embeddings\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\nüîç Test 3: Vector Embeddings\")\n",
        "    try:\n",
        "        test_text = \"This is a test of the TACITUS conflict analysis system.\"\n",
        "        embedding = vector_engine.embed_text(test_text)\n",
        "\n",
        "        assert len(embedding) == config.embedding_dimensions\n",
        "        assert any(v != 0 for v in embedding)  # Not all zeros\n",
        "\n",
        "        print(f\"   ‚úÖ Generated {len(embedding)}D embedding\")\n",
        "        print(f\"   Sample values: [{embedding[0]:.4f}, {embedding[1]:.4f}, ...]\")\n",
        "        results.append((\"Embeddings\", \"PASS\", f\"{len(embedding)}D\"))\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed: {e}\")\n",
        "        results.append((\"Embeddings\", \"FAIL\", str(e)))\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # TEST 4: Neo4j Vector Functions\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\nüîç Test 4: Neo4j Vector Functions\")\n",
        "    try:\n",
        "        with driver.session() as s:\n",
        "            # Test vector similarity function\n",
        "            result = s.run(\"\"\"\n",
        "                WITH [1.0, 0.0, 0.0] AS v1, [1.0, 0.0, 0.0] AS v2\n",
        "                RETURN vector.similarity.cosine(v1, v2) AS similarity\n",
        "            \"\"\").single()\n",
        "\n",
        "            sim = result['similarity']\n",
        "            assert sim == 1.0  # Identical vectors should have similarity 1.0\n",
        "\n",
        "        print(f\"   ‚úÖ Vector similarity working (test: {sim})\")\n",
        "        results.append((\"Neo4j Vectors\", \"PASS\", \"cosine similarity\"))\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Vector functions may not be available: {e}\")\n",
        "        print(f\"   This is OK - semantic search will fall back to basic queries\")\n",
        "        results.append((\"Neo4j Vectors\", \"WARN\", \"Using fallback\"))\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # TEST 5: FAISS\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\nüîç Test 5: FAISS Vector Index\")\n",
        "    try:\n",
        "        # Create a small test index\n",
        "        test_vectors = np.random.rand(10, 64).astype('float32')\n",
        "        faiss.normalize_L2(test_vectors)\n",
        "\n",
        "        index = faiss.IndexFlatIP(64)\n",
        "        index.add(test_vectors)\n",
        "\n",
        "        query = np.random.rand(1, 64).astype('float32')\n",
        "        faiss.normalize_L2(query)\n",
        "        scores, indices = index.search(query, 3)\n",
        "\n",
        "        print(f\"   ‚úÖ FAISS working - searched 10 vectors, found top 3\")\n",
        "        results.append((\"FAISS\", \"PASS\", \"Index working\"))\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed: {e}\")\n",
        "        results.append((\"FAISS\", \"FAIL\", str(e)))\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # SUMMARY\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä DIAGNOSTIC SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    df = pd.DataFrame(results, columns=['Component', 'Status', 'Details'])\n",
        "\n",
        "    # Color coding\n",
        "    def color_status(val):\n",
        "        if val == 'PASS':\n",
        "            return 'background-color: #90EE90'\n",
        "        elif val == 'FAIL':\n",
        "            return 'background-color: #FFB6C1'\n",
        "        else:\n",
        "            return 'background-color: #FFFFE0'\n",
        "\n",
        "    display(df.style.applymap(color_status, subset=['Status']).hide(axis='index'))\n",
        "\n",
        "    # Final verdict\n",
        "    failures = sum(1 for r in results if r[1] == 'FAIL')\n",
        "    if failures == 0:\n",
        "        print(\"\\nüéâ All systems operational! Ready to proceed.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è {failures} component(s) failed. Check errors above.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run diagnostics\n",
        "diagnostic_results = run_diagnostics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuBEg-mjVGhb"
      },
      "source": [
        "---\n",
        "# üß± BLOCK 4A: Apply Constraints & Indexes\n",
        "Run this once after your Neo4j connection works. It‚Äôs optional but strongly recommended for real datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXFIWSyCFRMz"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 4A: APPLY CONSTRAINTS / INDEXES\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "try:\n",
        "    ensure_neo4j_constraints(driver)\n",
        "except NameError:\n",
        "    print(\"‚ùå `driver` not found. Run BLOCK 4 first.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è DDL step had issues (often Neo4j version-related). Continuing. Details: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsF6wWD0xhEZ"
      },
      "source": [
        "---\n",
        "# üå± BLOCK 5: Seed Theories & Build OntoRAG Index\n",
        "**Populates the Grounding Graph (GND) with conflict theories.**\n",
        "\n",
        "This enables:\n",
        "- Smarter extraction (LLM knows what theories to look for)\n",
        "- Theory matching (link cases to relevant frameworks)\n",
        "- Cross-case analysis (find similar conflicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EisBVAoxhEa"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 5: THEORY SEEDING & ONTORAG INDEX\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Seed the GND graph with conflict theories and build search index\n",
        "# ‚è±Ô∏è TIME: ~60 seconds (API calls to generate theories)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# THEORY GENERATOR\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "SEED_PROMPT = \"\"\"You are an Expert Professor of Conflict Studies and Political Science.\n",
        "Generate a structured knowledge graph for this theoretical domain: '{domain}'.\n",
        "\n",
        "Include:\n",
        "1. The main theory with clear description\n",
        "2. 3-5 core concepts the theory uses\n",
        "3. Key causal relationships between concepts\n",
        "\n",
        "Return ONLY valid JSON matching this schema:\n",
        "{{\n",
        "  \"theories\": [{{\n",
        "    \"name\": \"Theory Name\",\n",
        "    \"description\": \"Clear 2-3 sentence description\",\n",
        "    \"proponent\": \"Key theorist(s)\",\n",
        "    \"core_concepts\": [\"concept1\", \"concept2\"]\n",
        "  }}],\n",
        "  \"concepts\": [{{\n",
        "    \"name\": \"Concept Name\",\n",
        "    \"definition\": \"Clear definition\",\n",
        "    \"concept_type\": \"phenomenon|mechanism|condition\",\n",
        "    \"indicators\": [\"observable indicator 1\", \"observable indicator 2\"]\n",
        "  }}],\n",
        "  \"causal_links\": [{{\n",
        "    \"source_concept\": \"Concept A\",\n",
        "    \"target_concept\": \"Concept B\",\n",
        "    \"relation_type\": \"causes|enables|prevents|correlates\",\n",
        "    \"justification\": \"Why this relationship exists\",\n",
        "    \"confidence\": 0.85\n",
        "  }}]\n",
        "}}\"\"\"\n",
        "\n",
        "class GNDWriter:\n",
        "    \"\"\"Writes grounding data to Neo4j.\"\"\"\n",
        "\n",
        "    def __init__(self, drv, vec_engine: VectorEngine = None):\n",
        "        self.driver = drv\n",
        "        self.vector_engine = vec_engine\n",
        "\n",
        "    def _get_embedding(self, text: str) -> Optional[List[float]]:\n",
        "        if self.vector_engine and text:\n",
        "            return self.vector_engine.embed_text(text[:500])\n",
        "        return None\n",
        "\n",
        "    def write(self, payload: GNDPayload, source_ref: str) -> Dict[str, int]:\n",
        "        \"\"\"Write GND data to Neo4j.\"\"\"\n",
        "        stats = defaultdict(int)\n",
        "\n",
        "        with self.driver.session() as s:\n",
        "            # 1. Concepts\n",
        "            for c in payload.concepts:\n",
        "                eid = f\"GND:CPT_{hashlib.md5(c.name.lower().strip().encode()).hexdigest()[:12]}\"\n",
        "                embedding = self._get_embedding(f\"{c.name}: {c.definition}\")\n",
        "\n",
        "                s.run('''\n",
        "                    MERGE (n:Concept {entity_id: $eid})\n",
        "                    SET n.name=$n, n.definition=$d, n.type=$ct,\n",
        "                        n.graph_category='GND', n.embedding=$emb,\n",
        "                        n.indicators=$ind\n",
        "                ''', eid=eid, n=c.name, d=c.definition, ct=c.concept_type,\n",
        "                     emb=embedding, ind=c.indicators)\n",
        "                stats['concepts'] += 1\n",
        "\n",
        "            # 2. Theories\n",
        "            for t in payload.theories:\n",
        "                eid = f\"GND:THY_{hashlib.md5(t.name.lower().strip().encode()).hexdigest()[:12]}\"\n",
        "                embedding = self._get_embedding(f\"{t.name}: {t.description}\")\n",
        "\n",
        "                s.run('''\n",
        "                    MERGE (th:Theory {entity_id: $eid})\n",
        "                    SET th.name=$n, th.desc=$d, th.proponent=$p,\n",
        "                        th.source=$src, th.graph_category='GND',\n",
        "                        th.embedding=$emb\n",
        "                ''', eid=eid, n=t.name, d=t.description, p=t.proponent,\n",
        "                     src=source_ref, emb=embedding)\n",
        "\n",
        "                # Link to concepts\n",
        "                for cn in t.core_concepts:\n",
        "                    s.run('''\n",
        "                        MATCH (th:Theory {entity_id: $tid})\n",
        "                        MATCH (c:Concept)\n",
        "                        WHERE toLower(c.name) = toLower($cn)\n",
        "                        MERGE (th)-[:EXPLAINS]->(c)\n",
        "                    ''', tid=eid, cn=cn)\n",
        "                stats['theories'] += 1\n",
        "\n",
        "            # 3. Causal Links\n",
        "            for link in payload.causal_links:\n",
        "                s.run('''\n",
        "                    MATCH (a:Concept), (b:Concept)\n",
        "                    WHERE toLower(a.name) = toLower($src) AND toLower(b.name) = toLower($tgt)\n",
        "                    MERGE (a)-[r:CAUSAL_LINK {type: $rel}]->(b)\n",
        "                    SET r.justification = $just, r.confidence = $conf\n",
        "                ''', src=link.source_concept, tgt=link.target_concept,\n",
        "                     rel=link.relation_type, just=link.justification,\n",
        "                     conf=link.confidence)\n",
        "                stats['causal_links'] += 1\n",
        "\n",
        "        return dict(stats)\n",
        "\n",
        "class TheorySeeder:\n",
        "    \"\"\"Seeds the GND graph with standard conflict theories.\"\"\"\n",
        "\n",
        "    # Standard conflict theories to seed\n",
        "    STANDARD_THEORIES = [\n",
        "        \"Relative Deprivation Theory (Ted Gurr) - why groups rebel when expectations exceed outcomes\",\n",
        "        \"Security Dilemma (Robert Jervis) - how defensive measures cause offensive responses\",\n",
        "        \"Resource Curse and Rentier State Theory - how natural resources fuel conflict\",\n",
        "        \"Social Movement Theory and Political Process - how movements mobilize and succeed\",\n",
        "        \"Elite Bargaining Theory - how power-sharing agreements prevent or end conflict\",\n",
        "        \"Ethnic Outbidding Theory - how politicians radicalize ethnic conflicts for votes\",\n",
        "        \"Commitment Problem in Civil Wars - why parties cannot credibly commit to peace\",\n",
        "        \"Greed vs Grievance Theory (Collier/Hoeffler) - economic motivations for rebellion\"\n",
        "    ]\n",
        "\n",
        "    def __init__(self, driver, vector_engine: VectorEngine):\n",
        "        self.driver = driver\n",
        "        self.vector_engine = vector_engine\n",
        "        self.model = GenerativeModel(config.extraction_model)\n",
        "        self.writer = GNDWriter(driver, vector_engine)\n",
        "        self.cfg = GenerationConfig(temperature=0.1, response_mime_type='application/json')\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
        "    def _generate_theory(self, domain: str) -> Optional[GNDPayload]:\n",
        "        \"\"\"Generate theory structure from LLM.\"\"\"\n",
        "        try:\n",
        "            prompt = SEED_PROMPT.format(domain=domain)\n",
        "            response = self.model.generate_content(prompt, generation_config=self.cfg)\n",
        "            data = json.loads(response.text)\n",
        "            return GNDPayload(**data)\n",
        "        except Exception as e:\n",
        "            console.print(f\"[yellow]   ‚ö†Ô∏è Generation failed: {e}[/yellow]\")\n",
        "            return None\n",
        "\n",
        "    def seed_all(self, theories: List[str] = None):\n",
        "        \"\"\"Seed all standard theories.\"\"\"\n",
        "        theories = theories or self.STANDARD_THEORIES\n",
        "\n",
        "        console.print(Panel(f\"[bold cyan]üå± Seeding {len(theories)} Conflict Theories[/bold cyan]\", border_style=\"cyan\"))\n",
        "\n",
        "        total_stats = defaultdict(int)\n",
        "\n",
        "        for i, domain in enumerate(theories, 1):\n",
        "            console.print(f\"\\n[{i}/{len(theories)}] {domain[:50]}...\")\n",
        "\n",
        "            payload = self._generate_theory(domain)\n",
        "            if payload:\n",
        "                stats = self.writer.write(payload, \"LLM_Knowledge_Seed\")\n",
        "                for k, v in stats.items():\n",
        "                    total_stats[k] += v\n",
        "                console.print(f\"   ‚úÖ Added: {stats}\")\n",
        "            else:\n",
        "                console.print(f\"   ‚ùå Failed\")\n",
        "\n",
        "            time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "        console.print(Panel(f\"[bold green]‚úÖ Seeding Complete: {dict(total_stats)}[/bold green]\", border_style=\"green\"))\n",
        "        return dict(total_stats)\n",
        "\n",
        "    def check_existing(self) -> int:\n",
        "        \"\"\"Check how many theories already exist.\"\"\"\n",
        "        with self.driver.session() as s:\n",
        "            result = s.run(\"MATCH (t:Theory {graph_category: 'GND'}) RETURN count(t) AS count\").single()\n",
        "            return result['count']\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERACTIVE SEEDING\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "seeder = TheorySeeder(driver, vector_engine)\n",
        "gnd_writer = GNDWriter(driver, vector_engine)\n",
        "\n",
        "existing_count = seeder.check_existing()\n",
        "print(f\"\\nüìä Current GND Graph: {existing_count} theories\")\n",
        "\n",
        "if existing_count == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No theories found. Click the button below to seed.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ {existing_count} theories already seeded.\")\n",
        "\n",
        "# Create UI\n",
        "btn_seed = widgets.Button(description=\"üå± Seed Standard Theories\", button_style='success', layout={'width': '250px'})\n",
        "btn_rebuild = widgets.Button(description=\"üîÑ Rebuild OntoRAG Index\", button_style='info', layout={'width': '250px'})\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_seed(_):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        seeder.seed_all()\n",
        "        print(\"\\nüîÑ Now rebuilding OntoRAG index...\")\n",
        "        onto_rag.build_index_from_neo4j(driver)\n",
        "\n",
        "def on_rebuild(_):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        onto_rag.build_index_from_neo4j(driver)\n",
        "\n",
        "btn_seed.on_click(on_seed)\n",
        "btn_rebuild.on_click(on_rebuild)\n",
        "\n",
        "display(widgets.HBox([btn_seed, btn_rebuild]))\n",
        "display(out)\n",
        "\n",
        "# Auto-build OntoRAG if theories exist\n",
        "if existing_count > 0:\n",
        "    print(\"\\nüîÑ Building OntoRAG index from existing theories...\")\n",
        "    onto_rag.build_index_from_neo4j(driver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq1s_HXExhEb"
      },
      "source": [
        "---\n",
        "# üìÑ BLOCK 6: Document Processing & Extraction Pipeline\n",
        "**Upload case files, extract entities, and populate the CTX graph.**\n",
        "\n",
        "This is the main extraction pipeline that:\n",
        "1. Uploads and chunks your documents\n",
        "2. Uses OntoRAG to select relevant theories\n",
        "3. Extracts actors, events, claims, etc.\n",
        "4. Resolves duplicate entities\n",
        "5. Writes everything to Neo4j with embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YebhdgwfxhEb"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 6: DOCUMENT PROCESSING & EXTRACTION PIPELINE\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Upload, process, and extract knowledge from case documents\n",
        "# ‚è±Ô∏è TIME: Varies by document size (~30s per 50KB)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# TEXT CHUNKER\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def chunk_text(text: str, doc_id: str, size: int = 20000, overlap: int = 1000) -> List[Chunk]:\n",
        "    \"\"\"Split text into overlapping chunks for processing.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    if len(text) <= size:\n",
        "        return [Chunk(f\"CHK_{doc_id[:8]}_0\", text, 0, 0, len(text))]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    idx = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = min(start + size, len(text))\n",
        "        chunk_content = text[start:end]\n",
        "\n",
        "        chunks.append(Chunk(\n",
        "            chunk_id=f\"CHK_{doc_id[:8]}_{idx}\",\n",
        "            content=chunk_content,\n",
        "            index=idx,\n",
        "            char_start=start,\n",
        "            char_end=end\n",
        "        ))\n",
        "\n",
        "        if end == len(text):\n",
        "            break\n",
        "\n",
        "        start = end - overlap\n",
        "        idx += 1\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_upload(filename: str, content: bytes, case_name: str) -> CaseDocument:\n",
        "    \"\"\"Convert uploaded file to CaseDocument.\"\"\"\n",
        "    text = content.decode('utf-8', errors='ignore')\n",
        "    sha256 = hashlib.sha256(text.encode()).hexdigest()\n",
        "\n",
        "    clean_name = re.sub(r'[^a-zA-Z0-9]', '_', case_name).upper()[:40]\n",
        "    case_id = f\"CTX_{clean_name}_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "\n",
        "    chunks = chunk_text(text, sha256, config.chunk_size, config.chunk_overlap)\n",
        "\n",
        "    return CaseDocument(case_id, case_name, filename, text, sha256, chunks)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# LLM EXTRACTION PROMPTS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are TACITUS, an expert conflict analyst. Extract structured data accurately.\n",
        "CRITICAL: Always extract dates/timelines. If no explicit date, infer from context.\n",
        "Return ONLY valid JSON matching the schema exactly.\"\"\"\n",
        "\n",
        "def build_extraction_prompt(text: str, stage: str, relevant_theories: List[Dict] = None) -> str:\n",
        "    \"\"\"Build extraction prompt with optional OntoRAG context.\"\"\"\n",
        "\n",
        "    theory_context = \"\"\n",
        "    if relevant_theories:\n",
        "        theory_context = \"\\n\\nRELEVANT THEORETICAL FRAMEWORKS (use to guide extraction):\\n\"\n",
        "        for t in relevant_theories[:5]:\n",
        "            theory_context += f\"‚Ä¢ {t['name']}: {t['description'][:150]}...\\n\"\n",
        "\n",
        "    if stage == \"actors\":\n",
        "        return f\"\"\"{SYSTEM_PROMPT}{theory_context}\n",
        "\n",
        "Extract ACTORS and RELATIONSHIPS from this conflict text.\n",
        "\n",
        "JSON Schema:\n",
        "{{\n",
        "  \"actors\": [{{\n",
        "    \"name\": \"string (full name)\",\n",
        "    \"actor_type\": \"person|org|state|group\",\n",
        "    \"role\": \"string (their role in conflict)\",\n",
        "    \"description\": \"string (brief description)\",\n",
        "    \"is_primary\": boolean,\n",
        "    \"confidence\": float 0-1\n",
        "  }}],\n",
        "  \"relationships\": [{{\n",
        "    \"source\": \"actor name\",\n",
        "    \"target\": \"actor name\",\n",
        "    \"rel_type\": \"OPPOSES|ALLIES_WITH|INFLUENCES|DEPENDS_ON\",\n",
        "    \"valid_from\": \"YYYY-MM-DD or null\",\n",
        "    \"status\": \"active|historical\",\n",
        "    \"confidence\": float 0-1\n",
        "  }}]\n",
        "}}\n",
        "\n",
        "TEXT:\n",
        "{text[:25000]}\"\"\"\n",
        "\n",
        "    else:  # details stage\n",
        "        return f\"\"\"{SYSTEM_PROMPT}{theory_context}\n",
        "\n",
        "Extract EVENTS, CLAIMS, INTERESTS, LEVERAGE, CONSTRAINTS, COMMITMENTS, and NARRATIVES.\n",
        "\n",
        "JSON Schema:\n",
        "{{\n",
        "  \"events\": [{{\n",
        "    \"description\": \"string\",\n",
        "    \"event_type\": \"action|escalation|negotiation|agreement\",\n",
        "    \"date\": \"YYYY-MM-DD or null\",\n",
        "    \"actor_names\": [\"string\"],\n",
        "    \"impact_level\": \"critical|high|moderate|low\",\n",
        "    \"confidence\": float\n",
        "  }}],\n",
        "  \"claims\": [{{\n",
        "    \"statement\": \"string (the claim)\",\n",
        "    \"actor_name\": \"who made it\",\n",
        "    \"claim_type\": \"demand|threat|accusation|promise\",\n",
        "    \"target_actor\": \"string or null\",\n",
        "    \"valid_from\": \"YYYY-MM-DD or null\",\n",
        "    \"confidence\": float\n",
        "  }}],\n",
        "  \"interests\": [{{\n",
        "    \"description\": \"string\",\n",
        "    \"actor_name\": \"string\",\n",
        "    \"interest_type\": \"security|political|economic|identity\",\n",
        "    \"confidence\": float\n",
        "  }}],\n",
        "  \"leverage\": [{{\n",
        "    \"description\": \"string\",\n",
        "    \"actor_name\": \"who holds it\",\n",
        "    \"leverage_type\": \"coercive|economic|informational|structural\",\n",
        "    \"target_actors\": [\"string\"],\n",
        "    \"confidence\": float\n",
        "  }}],\n",
        "  \"constraints\": [{{\n",
        "    \"description\": \"string\",\n",
        "    \"actor_name\": \"string or null\",\n",
        "    \"constraint_type\": \"political|legal|economic|resource\",\n",
        "    \"confidence\": float\n",
        "  }}],\n",
        "  \"commitments\": [{{\n",
        "    \"description\": \"string\",\n",
        "    \"parties\": [\"string\"],\n",
        "    \"status\": \"active|broken|fulfilled\",\n",
        "    \"confidence\": float\n",
        "  }}],\n",
        "  \"narratives\": [{{\n",
        "    \"description\": \"string\",\n",
        "    \"actor_name\": \"who promotes it\",\n",
        "    \"frame_type\": \"victim|aggressor|mediator|defender\",\n",
        "    \"confidence\": float\n",
        "  }}]\n",
        "}}\n",
        "\n",
        "TEXT:\n",
        "{text[:25000]}\"\"\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# ASYNC EXTRACTOR\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "class AsyncExtractor:\n",
        "    \"\"\"Async extraction with OntoRAG support.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, onto_rag_selector: OntoRAGSelector = None):\n",
        "        self.model = GenerativeModel(model_name)\n",
        "        self.cfg = GenerationConfig(temperature=0.0, response_mime_type='application/json')\n",
        "        self.onto_rag = onto_rag_selector\n",
        "\n",
        "    async def _call_llm(self, prompt: str) -> Dict:\n",
        "        try:\n",
        "            response = await asyncio.to_thread(\n",
        "                self.model.generate_content, prompt, generation_config=self.cfg\n",
        "            )\n",
        "            return json.loads(response.text)\n",
        "        except Exception as e:\n",
        "            console.print(f\"[yellow]‚ö†Ô∏è LLM error: {e}[/yellow]\")\n",
        "            return {}\n",
        "\n",
        "    def _sanitize(self, data_list: list, required_field: str = None) -> list:\n",
        "        \"\"\"Clean LLM output to prevent validation errors.\"\"\"\n",
        "        if not isinstance(data_list, list):\n",
        "            return []\n",
        "\n",
        "        clean = []\n",
        "        for item in data_list:\n",
        "            if not isinstance(item, dict):\n",
        "                continue\n",
        "\n",
        "            # Fix missing required fields\n",
        "            if required_field and not item.get(required_field):\n",
        "                item[required_field] = \"Unknown\"\n",
        "\n",
        "            # Fix null dates\n",
        "            for date_field in ['date', 'valid_from', 'valid_until']:\n",
        "                if date_field in item and item[date_field] is None:\n",
        "                    item[date_field] = config.default_date\n",
        "\n",
        "            # Ensure target_actor for claims\n",
        "            if 'claim_type' in item and 'target_actor' not in item:\n",
        "                item['target_actor'] = None\n",
        "\n",
        "            clean.append(item)\n",
        "\n",
        "        return clean\n",
        "\n",
        "    async def extract_chunk(self, content: str) -> FullExtraction:\n",
        "        \"\"\"Extract all entities from a chunk.\"\"\"\n",
        "\n",
        "        # Get relevant theories via OntoRAG\n",
        "        relevant_theories = []\n",
        "        if self.onto_rag and self.onto_rag.is_initialized:\n",
        "            relevant_theories = self.onto_rag.select_relevant_theories(content[:3000], top_k=3)\n",
        "\n",
        "        # Build prompts\n",
        "        prompt_actors = build_extraction_prompt(content, 'actors', relevant_theories)\n",
        "        prompt_details = build_extraction_prompt(content, 'details', relevant_theories)\n",
        "\n",
        "        # Parallel extraction\n",
        "        d1, d2 = await asyncio.gather(\n",
        "            self._call_llm(prompt_actors),\n",
        "            self._call_llm(prompt_details)\n",
        "        )\n",
        "\n",
        "        ext = FullExtraction()\n",
        "\n",
        "        # Parse actors\n",
        "        ext.actors = [ExtractedActor(**a) for a in self._sanitize(d1.get('actors', []), 'name')]\n",
        "        ext.relationships = [ActorRelationship(**r) for r in self._sanitize(d1.get('relationships', []), 'source')]\n",
        "\n",
        "        # Parse details\n",
        "        ext.events = [ExtractedEvent(**e) for e in self._sanitize(d2.get('events', []), 'description')]\n",
        "        ext.claims = [ExtractedClaim(**c) for c in self._sanitize(d2.get('claims', []), 'actor_name')]\n",
        "        ext.interests = [ExtractedInterest(**i) for i in self._sanitize(d2.get('interests', []), 'actor_name')]\n",
        "        ext.leverage = [ExtractedLeverage(**l) for l in self._sanitize(d2.get('leverage', []), 'actor_name')]\n",
        "        ext.constraints = [ExtractedConstraint(**c) for c in self._sanitize(d2.get('constraints', []), 'description')]\n",
        "        ext.commitments = [ExtractedCommitment(**cm) for cm in self._sanitize(d2.get('commitments', []), 'description')]\n",
        "        ext.narratives = [ExtractedNarrative(**n) for n in self._sanitize(d2.get('narratives', []), 'actor_name')]\n",
        "\n",
        "        return ext\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# ENTITY RESOLVER\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "class EntityResolver:\n",
        "    \"\"\"Resolves duplicate entities using fuzzy matching.\"\"\"\n",
        "\n",
        "    def __init__(self, threshold: float = 85.0):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def normalize(self, name: str) -> str:\n",
        "        name = name.lower().strip()\n",
        "        for prefix in ['mr. ', 'mrs. ', 'dr. ', 'the ', 'president ']:\n",
        "            if name.startswith(prefix):\n",
        "                name = name[len(prefix):]\n",
        "        return re.sub(r'\\s+', ' ', name)\n",
        "\n",
        "    def find_duplicates(self, actors: List[ExtractedActor]) -> Dict[str, List[str]]:\n",
        "        \"\"\"Find groups of duplicate actors.\"\"\"\n",
        "        dupes = defaultdict(list)\n",
        "        processed = set()\n",
        "\n",
        "        for i, a1 in enumerate(actors):\n",
        "            if a1.name in processed:\n",
        "                continue\n",
        "\n",
        "            group = [a1.name]\n",
        "            for a2 in actors[i+1:]:\n",
        "                if a2.name in processed:\n",
        "                    continue\n",
        "\n",
        "                score = max(\n",
        "                    fuzz.ratio(self.normalize(a1.name), self.normalize(a2.name)),\n",
        "                    fuzz.token_sort_ratio(self.normalize(a1.name), self.normalize(a2.name))\n",
        "                )\n",
        "\n",
        "                if score >= self.threshold:\n",
        "                    group.append(a2.name)\n",
        "                    processed.add(a2.name)\n",
        "\n",
        "            if len(group) > 1:\n",
        "                canonical = max(group, key=len)  # Use longest name as canonical\n",
        "                dupes[canonical] = [n for n in group if n != canonical]\n",
        "\n",
        "            processed.add(a1.name)\n",
        "\n",
        "        return dict(dupes)\n",
        "\n",
        "    def merge(self, actors: List[ExtractedActor]) -> Tuple[List[ExtractedActor], Dict[str, str]]:\n",
        "        \"\"\"Merge duplicate actors and return mapping.\"\"\"\n",
        "        dupes = self.find_duplicates(actors)\n",
        "        mapping = {d: c for c, ds in dupes.items() for d in ds}\n",
        "\n",
        "        merged = {}\n",
        "        for a in actors:\n",
        "            canonical = mapping.get(a.name, a.name)\n",
        "            if canonical not in merged:\n",
        "                merged[canonical] = ExtractedActor(\n",
        "                    name=canonical, actor_type=a.actor_type, role=a.role,\n",
        "                    description=a.description, is_primary=a.is_primary,\n",
        "                    aliases=list(set(a.aliases + ([a.name] if a.name != canonical else []))),\n",
        "                    confidence=a.confidence\n",
        "                )\n",
        "            else:\n",
        "                m = merged[canonical]\n",
        "                m.is_primary = m.is_primary or a.is_primary\n",
        "                m.aliases = list(set(m.aliases + [a.name]))\n",
        "                m.confidence = max(m.confidence, a.confidence)\n",
        "\n",
        "        return list(merged.values()), mapping\n",
        "\n",
        "    def apply_mapping(self, ext: FullExtraction, mapping: Dict[str, str]) -> FullExtraction:\n",
        "        \"\"\"Apply name mapping to all entities.\"\"\"\n",
        "        def m(n): return mapping.get(n, n) if n else n\n",
        "\n",
        "        for c in ext.claims:\n",
        "            c.actor_name = m(c.actor_name)\n",
        "            c.target_actor = m(c.target_actor)\n",
        "        for i in ext.interests:\n",
        "            i.actor_name = m(i.actor_name)\n",
        "        for l in ext.leverage:\n",
        "            l.actor_name = m(l.actor_name)\n",
        "            l.target_actors = [m(t) for t in l.target_actors]\n",
        "        for e in ext.events:\n",
        "            e.actor_names = [m(a) for a in e.actor_names]\n",
        "        for c in ext.constraints:\n",
        "            c.actor_name = m(c.actor_name)\n",
        "        for cm in ext.commitments:\n",
        "            cm.parties = [m(p) for p in cm.parties]\n",
        "        for n in ext.narratives:\n",
        "            n.actor_name = m(n.actor_name)\n",
        "        for r in ext.relationships:\n",
        "            r.source = m(r.source)\n",
        "            r.target = m(r.target)\n",
        "\n",
        "        return ext\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# GRAPH WRITER (with embeddings)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "# Allowed relationship types (for Cypher safety)\n",
        "ALLOWED_REL_TYPES = {'OPPOSES', 'ALLIES_WITH', 'INFLUENCES', 'DEPENDS_ON', 'RELATIONSHIP'}\n",
        "\n",
        "class GraphWriter:\n",
        "    \"\"\"Writes extracted data to Neo4j with embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, drv, vec_engine: VectorEngine = None):\n",
        "        self.driver = drv\n",
        "        self.vector_engine = vec_engine\n",
        "\n",
        "    def _embed(self, text: str) -> Optional[List[float]]:\n",
        "        if self.vector_engine and text:\n",
        "            return self.vector_engine.embed_text(text[:500])\n",
        "        return None\n",
        "\n",
        "    def write_case(self, case_id: str, case_name: str):\n",
        "        \"\"\"Create case node.\"\"\"\n",
        "        embedding = self._embed(case_name)\n",
        "        with self.driver.session() as s:\n",
        "            s.run('''\n",
        "                MERGE (c:Case {case_id: $cid})\n",
        "                ON CREATE SET c.created_at = datetime()\n",
        "                SET c.case_name=$cn, c.graph_category='CTX',\n",
        "                    c.updated_at=datetime(), c.embedding=$emb\n",
        "            ''', cid=case_id, cn=case_name, emb=embedding)\n",
        "\n",
        "    def write_extraction(self, ext: FullExtraction, case_id: str) -> Dict[str, int]:\n",
        "        \"\"\"Write all extracted entities to Neo4j.\"\"\"\n",
        "        stats = defaultdict(int)\n",
        "\n",
        "        with self.driver.session() as s:\n",
        "            # ACTORS\n",
        "            for a in ext.actors:\n",
        "                eid = f\"CTX:ACT_{hashlib.md5(f'{case_id}:{a.name.lower()}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(f\"{a.name}: {a.role or ''} {a.description or ''}\")\n",
        "                s.run('''\n",
        "                    MERGE (a:Actor {entity_id: $eid})\n",
        "                    SET a.name=$n, a.role=$r, a.type=$t, a.description=$d,\n",
        "                        a.is_primary=$p, a.case_id=$cid, a.graph_category='CTX',\n",
        "                        a.embedding=$emb, a.aliases=$aliases\n",
        "                    WITH a MATCH (c:Case {case_id: $cid}) MERGE (c)-[:CONTAINS]->(a)\n",
        "                ''', eid=eid, n=a.name, r=a.role, t=a.actor_type, d=a.description,\n",
        "                     p=a.is_primary, cid=case_id, emb=emb, aliases=a.aliases)\n",
        "                stats['actors'] += 1\n",
        "\n",
        "            # EVENTS\n",
        "            for e in ext.events:\n",
        "                eid = f\"CTX:EVT_{hashlib.md5(f'{case_id}:{e.description[:30]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(e.description)\n",
        "                s.run('''\n",
        "                    MERGE (ev:Event {entity_id: $eid})\n",
        "                    SET ev.description=$d, ev.date=$dt, ev.type=$et, ev.impact=$il,\n",
        "                        ev.case_id=$cid, ev.graph_category='CTX', ev.embedding=$emb\n",
        "                ''', eid=eid, d=e.description, dt=e.date, et=e.event_type,\n",
        "                     il=e.impact_level, cid=case_id, emb=emb)\n",
        "                for an in e.actor_names:\n",
        "                    s.run('MATCH (ev:Event {entity_id: $eid}), (a:Actor {name: $an, case_id: $cid}) MERGE (a)-[:INITIATED_EVENT]->(ev)',\n",
        "                          eid=eid, an=an, cid=case_id)\n",
        "                stats['events'] += 1\n",
        "\n",
        "            # RELATIONSHIPS\n",
        "            for r in ext.relationships:\n",
        "                rel_type = r.rel_type if r.rel_type in ALLOWED_REL_TYPES else 'RELATIONSHIP'\n",
        "                s.run(f'''\n",
        "                    MATCH (a:Actor {{name: $s, case_id: $cid}})\n",
        "                    MATCH (b:Actor {{name: $t, case_id: $cid}})\n",
        "                    MERGE (a)-[rel:{rel_type}]->(b)\n",
        "                    SET rel.valid_from=$vf, rel.status=$st, rel.confidence=$conf\n",
        "                ''', s=r.source, t=r.target, vf=r.valid_from, st=r.status,\n",
        "                     conf=r.confidence, cid=case_id)\n",
        "                stats['relationships'] += 1\n",
        "\n",
        "            # CLAIMS\n",
        "            for c in ext.claims:\n",
        "                eid = f\"CTX:CLM_{hashlib.md5(f'{case_id}:{c.statement[:20]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(c.statement)\n",
        "                s.run('''\n",
        "                    MERGE (n:Claim {entity_id: $eid})\n",
        "                    SET n.statement=$st, n.type=$ct, n.target_actor=$ta, n.valid_from=$vf,\n",
        "                        n.case_id=$cid, n.graph_category='CTX', n.embedding=$emb\n",
        "                ''', eid=eid, st=c.statement, ct=c.claim_type, ta=c.target_actor,\n",
        "                     vf=c.valid_from, cid=case_id, emb=emb)\n",
        "                s.run('MATCH (n:Claim {entity_id: $eid}), (a:Actor {name: $an, case_id: $cid}) MERGE (a)-[:MAKES_CLAIM]->(n)',\n",
        "                      eid=eid, an=c.actor_name, cid=case_id)\n",
        "                stats['claims'] += 1\n",
        "\n",
        "            # INTERESTS\n",
        "            for i in ext.interests:\n",
        "                eid = f\"CTX:INT_{hashlib.md5(f'{case_id}:{i.description[:20]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(i.description)\n",
        "                s.run('''\n",
        "                    MERGE (n:Interest {entity_id: $eid})\n",
        "                    SET n.description=$d, n.type=$t, n.case_id=$cid, n.graph_category='CTX', n.embedding=$emb\n",
        "                ''', eid=eid, d=i.description, t=i.interest_type, cid=case_id, emb=emb)\n",
        "                s.run('MATCH (n:Interest {entity_id: $eid}), (a:Actor {name: $an, case_id: $cid}) MERGE (a)-[:HAS_INTEREST]->(n)',\n",
        "                      eid=eid, an=i.actor_name, cid=case_id)\n",
        "                stats['interests'] += 1\n",
        "\n",
        "            # LEVERAGE\n",
        "            for l in ext.leverage:\n",
        "                eid = f\"CTX:LEV_{hashlib.md5(f'{case_id}:{l.description[:20]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(l.description)\n",
        "                s.run('''\n",
        "                    MERGE (n:Leverage {entity_id: $eid})\n",
        "                    SET n.description=$d, n.type=$t, n.case_id=$cid, n.graph_category='CTX', n.embedding=$emb\n",
        "                ''', eid=eid, d=l.description, t=l.leverage_type, cid=case_id, emb=emb)\n",
        "                s.run('MATCH (n:Leverage {entity_id: $eid}), (a:Actor {name: $an, case_id: $cid}) MERGE (a)-[:HOLDS_LEVERAGE]->(n)',\n",
        "                      eid=eid, an=l.actor_name, cid=case_id)\n",
        "                stats['leverage'] += 1\n",
        "\n",
        "            # CONSTRAINTS\n",
        "            for c in ext.constraints:\n",
        "                eid = f\"CTX:CON_{hashlib.md5(f'{case_id}:{c.description[:20]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(c.description)\n",
        "                s.run('''\n",
        "                    MERGE (n:Constraint {entity_id: $eid})\n",
        "                    SET n.description=$d, n.type=$t, n.case_id=$cid, n.graph_category='CTX', n.embedding=$emb\n",
        "                ''', eid=eid, d=c.description, t=c.constraint_type, cid=case_id, emb=emb)\n",
        "                if c.actor_name:\n",
        "                    s.run('MATCH (n:Constraint {entity_id: $eid}), (a:Actor {name: $an, case_id: $cid}) MERGE (a)-[:FACES_CONSTRAINT]->(n)',\n",
        "                          eid=eid, an=c.actor_name, cid=case_id)\n",
        "                stats['constraints'] += 1\n",
        "\n",
        "            # COMMITMENTS\n",
        "            for cm in ext.commitments:\n",
        "                eid = f\"CTX:CMT_{hashlib.md5(f'{case_id}:{cm.description[:20]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(cm.description)\n",
        "                s.run('''\n",
        "                    MERGE (n:Commitment {entity_id: $eid})\n",
        "                    SET n.description=$d, n.status=$st, n.case_id=$cid, n.graph_category='CTX', n.embedding=$emb\n",
        "                ''', eid=eid, d=cm.description, st=cm.status, cid=case_id, emb=emb)\n",
        "                for p in cm.parties:\n",
        "                    s.run('MATCH (n:Commitment {entity_id: $eid}), (a:Actor {name: $p, case_id: $cid}) MERGE (a)-[:MADE_COMMITMENT]->(n)',\n",
        "                          eid=eid, p=p, cid=case_id)\n",
        "                stats['commitments'] += 1\n",
        "\n",
        "            # NARRATIVES\n",
        "            for n in ext.narratives:\n",
        "                eid = f\"CTX:NAR_{hashlib.md5(f'{case_id}:{n.description[:20]}'.encode()).hexdigest()[:12]}\"\n",
        "                emb = self._embed(n.description)\n",
        "                s.run('''\n",
        "                    MERGE (node:Narrative {entity_id: $eid})\n",
        "                    SET node.description=$d, node.frame=$f, node.case_id=$cid, node.graph_category='CTX', node.embedding=$emb\n",
        "                ''', eid=eid, d=n.description, f=n.frame_type, cid=case_id, emb=emb)\n",
        "                s.run('MATCH (node:Narrative {entity_id: $eid}), (a:Actor {name: $an, case_id: $cid}) MERGE (a)-[:PROMOTES_NARRATIVE]->(node)',\n",
        "                      eid=eid, an=n.actor_name, cid=case_id)\n",
        "                stats['narratives'] += 1\n",
        "\n",
        "        return dict(stats)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INITIALIZE COMPONENTS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "extractor = AsyncExtractor(config.extraction_model, onto_rag)\n",
        "resolver = EntityResolver(config.er_threshold)\n",
        "writer = GraphWriter(driver, vector_engine)\n",
        "\n",
        "print(\"‚úÖ Extraction pipeline ready:\")\n",
        "print(f\"   ‚Ä¢ Extractor: {config.extraction_model}\")\n",
        "print(f\"   ‚Ä¢ Entity Resolver: {config.er_threshold}% threshold\")\n",
        "print(f\"   ‚Ä¢ Graph Writer: Embeddings {'enabled' if vector_engine else 'disabled'}\")\n",
        "print(f\"   ‚Ä¢ OntoRAG: {'active' if onto_rag.is_initialized else 'inactive (seed theories first)'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZFDG3xAwPmx"
      },
      "source": [
        "---\n",
        "# üßΩ BLOCK 6A: Dataset Hygiene & Text De-noising\n",
        "**Goal:** Strip boilerplate/noise (e.g., *Project Gutenberg* headers/footers) before chunking + extraction, and keep an audit trail.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnE_LTJpwPmy"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 6A: DATASET HYGIENE & TEXT DE-NOISING\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "@dataclass\n",
        "class CleaningHit:\n",
        "    rule: str\n",
        "    count: int\n",
        "    examples: List[str]\n",
        "\n",
        "# Common boilerplate patterns (add your own!)\n",
        "CLEANING_RULES = [\n",
        "    {\n",
        "        \"name\": \"project_gutenberg_block\",\n",
        "        \"pattern\": r\"(?is)\\*\\*\\*\\s*start of (this|the) project gutenberg.*?\\*\\*\\*\\s*end of (this|the) project gutenberg.*?\\*\\*\\*\",\n",
        "        \"replace_with\": \"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"project_gutenberg_lines\",\n",
        "        \"pattern\": r\"(?im)^\\s*(?:this\\s+ebook\\s+is\\s+for\\s+the\\s+use\\s+of\\s+anyone\\s+anywhere.*|project\\s+gutenberg.*|end\\s+of\\s+(?:this|the)\\s+project\\s+gutenberg.*)\\s*$\",\n",
        "        \"replace_with\": \"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"excess_whitespace\",\n",
        "        \"pattern\": r\"\\s+\",\n",
        "        \"replace_with\": \" \"\n",
        "    },\n",
        "]\n",
        "\n",
        "def clean_text(raw: str, max_examples: int = 3) -> Tuple[str, List[CleaningHit]]:\n",
        "    \"\"\"\n",
        "    Apply deterministic cleaning rules and return an audit trail.\n",
        "    Keeps this intentionally conservative: it's better to leave some noise than to delete evidence.\n",
        "    \"\"\"\n",
        "    text = raw or \"\"\n",
        "    hits: List[CleaningHit] = []\n",
        "\n",
        "    for rule in CLEANING_RULES:\n",
        "        before = text\n",
        "        text, n = re.subn(rule[\"pattern\"], rule[\"replace_with\"], text)\n",
        "        if n > 0:\n",
        "            # Capture a few tiny examples from the *before* text to make it debuggable\n",
        "            ex = []\n",
        "            try:\n",
        "                for m in re.finditer(rule[\"pattern\"], before):\n",
        "                    snippet = before[m.start():min(m.start()+120, len(before))]\n",
        "                    ex.append(snippet.replace(\"\\n\", \" \")[:120])\n",
        "                    if len(ex) >= max_examples:\n",
        "                        break\n",
        "            except re.error:\n",
        "                pass\n",
        "            hits.append(CleaningHit(rule=rule[\"name\"], count=n, examples=ex))\n",
        "\n",
        "    return text.strip(), hits\n",
        "\n",
        "def lint_text(raw: str) -> Dict[str, bool]:\n",
        "    \"\"\"Lightweight flags that help you spot obvious dataset artifacts.\"\"\"\n",
        "    flags = {\n",
        "        \"contains_project_gutenberg\": bool(re.search(r\"(?i)project\\s+gutenberg\", raw or \"\")),\n",
        "        \"contains_long_url\": bool(re.search(r\"https?://\\S{40,}\", raw or \"\")),\n",
        "        \"contains_repeated_dashes\": bool(re.search(r\"-{40,}\", raw or \"\")),\n",
        "    }\n",
        "    return flags\n",
        "\n",
        "# Wrap process_upload so you don't need to edit BLOCK 6 code above\n",
        "_original_process_upload = process_upload\n",
        "\n",
        "def process_upload(filename: str, content: bytes, case_name: str) -> CaseDocument:\n",
        "    \"\"\"Convert uploaded file to CaseDocument with cleaning + linting.\"\"\"\n",
        "    text = content.decode('utf-8', errors='ignore')\n",
        "\n",
        "    flags = lint_text(text)\n",
        "    cleaned, audit = clean_text(text)\n",
        "\n",
        "    if audit:\n",
        "        console.print(f\"[cyan]üßΩ Cleaned {filename}[/cyan]\")\n",
        "        for h in audit:\n",
        "            console.print(f\"   ‚Ä¢ {h.rule}: {h.count} hit(s)\")\n",
        "    if any(flags.values()):\n",
        "        console.print(f\"[yellow]üîé Lint flags for {filename}: {', '.join([k for k,v in flags.items() if v])}[/yellow]\")\n",
        "\n",
        "    # Use cleaned text downstream\n",
        "    sha256 = hashlib.sha256(cleaned.encode()).hexdigest()\n",
        "\n",
        "    clean_name = re.sub(r'[^a-zA-Z0-9]', '_', case_name).upper()[:40]\n",
        "    case_id = f\"CTX_{clean_name}_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "\n",
        "    chunks = chunk_text(cleaned, sha256, config.chunk_size, config.chunk_overlap)\n",
        "\n",
        "    return CaseDocument(case_id, case_name, filename, cleaned, sha256, chunks)\n",
        "\n",
        "console.print(\"[green]‚úÖ Dataset hygiene enabled (process_upload wrapped).[/green]\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMawoNswxhEf"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# BATCH PROCESSING PIPELINE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "PROCESS_QUEUE = []  # Global queue for documents\n",
        "\n",
        "async def process_document(doc: CaseDocument) -> Dict[str, int]:\n",
        "    \"\"\"Process a single document through the full pipeline.\"\"\"\n",
        "    console.print(f\"\\n[bold cyan]‚ñ∂Ô∏è Processing: {doc.case_name}[/bold cyan] ({len(doc.chunks)} chunks)\")\n",
        "\n",
        "    # Extract from all chunks in parallel\n",
        "    tasks = [extractor.extract_chunk(chunk.content) for chunk in doc.chunks]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Merge all extractions\n",
        "    merged = FullExtraction()\n",
        "    for ext in results:\n",
        "        merged.actors.extend(ext.actors)\n",
        "        merged.relationships.extend(ext.relationships)\n",
        "        merged.events.extend(ext.events)\n",
        "        merged.claims.extend(ext.claims)\n",
        "        merged.interests.extend(ext.interests)\n",
        "        merged.leverage.extend(ext.leverage)\n",
        "        merged.constraints.extend(ext.constraints)\n",
        "        merged.commitments.extend(ext.commitments)\n",
        "        merged.narratives.extend(ext.narratives)\n",
        "\n",
        "    console.print(f\"   üìä Raw extraction: {merged.total_entities()} entities\")\n",
        "\n",
        "    # Entity resolution\n",
        "    if merged.actors:\n",
        "        merged.actors, mapping = resolver.merge(merged.actors)\n",
        "        if mapping:\n",
        "            merged = resolver.apply_mapping(merged, mapping)\n",
        "            console.print(f\"   üîó Resolved {len(mapping)} duplicate actors\")\n",
        "\n",
        "    # Write to graph\n",
        "    if not config.dry_run:\n",
        "        writer.write_case(doc.case_id, doc.case_name)\n",
        "        stats = writer.write_extraction(merged, doc.case_id)\n",
        "        return stats\n",
        "\n",
        "    return merged.stats()\n",
        "\n",
        "async def run_batch_pipeline():\n",
        "    \"\"\"Run the full pipeline on all queued documents.\"\"\"\n",
        "    if not PROCESS_QUEUE:\n",
        "        console.print(\"[yellow]‚ö†Ô∏è No documents in queue. Upload files first.[/yellow]\")\n",
        "        return\n",
        "\n",
        "    console.print(Panel(f\"[bold cyan]üöÄ Processing {len(PROCESS_QUEUE)} Document(s)[/bold cyan]\", border_style=\"cyan\"))\n",
        "\n",
        "    total_start = time.time()\n",
        "    all_stats = defaultdict(int)\n",
        "\n",
        "    for doc in PROCESS_QUEUE:\n",
        "        try:\n",
        "            start = time.time()\n",
        "            stats = await process_document(doc)\n",
        "            duration = time.time() - start\n",
        "\n",
        "            for k, v in stats.items():\n",
        "                all_stats[k] += v\n",
        "\n",
        "            console.print(f\"   ‚úÖ Done in {duration:.1f}s: {stats}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            console.print(f\"   ‚ùå Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "\n",
        "    console.print(Panel(\n",
        "        f\"[bold green]‚ú® Complete![/bold green]\\n\" +\n",
        "        f\"Time: {total_time:.1f}s | Totals: {dict(all_stats)}\",\n",
        "        border_style=\"green\"\n",
        "    ))\n",
        "\n",
        "    # Show embedding stats\n",
        "    if vector_engine:\n",
        "        console.print(f\"\\nüìä Embedding Stats: {vector_engine.stats()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìÑ UPLOAD YOUR CASE FILES\")\n",
        "print(\"=\"*60)\n",
        "print(\"Upload .txt files containing conflict case descriptions.\")\n",
        "print(\"\")\n",
        "\n",
        "# File upload widget\n",
        "uploader = widgets.FileUpload(accept='.txt', multiple=True, description='Upload .txt')\n",
        "btn_process = widgets.Button(description='üöÄ Process All Files', button_style='success', layout={'width': '200px'})\n",
        "out_upload = widgets.Output()\n",
        "\n",
        "def on_upload_change(change):\n",
        "    global PROCESS_QUEUE\n",
        "    with out_upload:\n",
        "        clear_output()\n",
        "        PROCESS_QUEUE = []\n",
        "\n",
        "        for filename, file_info in uploader.value.items():\n",
        "            content = file_info['content']\n",
        "            size_kb = len(content) / 1024\n",
        "\n",
        "            # Auto-generate case name from filename\n",
        "            case_name = os.path.splitext(filename)[0].replace('_', ' ').title()\n",
        "\n",
        "            doc = process_upload(filename, content, case_name)\n",
        "            PROCESS_QUEUE.append(doc)\n",
        "\n",
        "            print(f\"üìÑ {filename} ({size_kb:.1f} KB) ‚Üí {len(doc.chunks)} chunks\")\n",
        "\n",
        "        print(f\"\\n‚úÖ {len(PROCESS_QUEUE)} file(s) ready. Click 'Process All Files' to extract.\")\n",
        "\n",
        "def on_process_click(_):\n",
        "    with out_upload:\n",
        "        asyncio.get_event_loop().run_until_complete(run_batch_pipeline())\n",
        "\n",
        "uploader.observe(on_upload_change, names='value')\n",
        "btn_process.on_click(on_process_click)\n",
        "\n",
        "display(widgets.VBox([uploader, btn_process, out_upload]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urhPs3LmwPmz"
      },
      "source": [
        "---\n",
        "# üß© BLOCK 6B: Entity Resolution Workbench (Human-in-the-loop Dedup)\n",
        "**Goal:** Suggest likely duplicate Actors (within a case) using blocking + fuzzy/semantic similarity, so you can review merges before you hard-merge anything.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FONwSDZYwPm0"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 6B: ENTITY RESOLUTION WORKBENCH (HITL DEDUP)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from rapidfuzz import fuzz\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "def _norm_name(s: str) -> str:\n",
        "    s = (s or \"\").lower().strip()\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)          # drop punctuation\n",
        "    s = re.sub(r\"\\b(inc|llc|ltd|corp|co)\\b\", \"\", s)  # org suffixes\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _blocking_keys(name: str) -> List[str]:\n",
        "    n = _norm_name(name)\n",
        "    toks = n.split()\n",
        "    if not toks:\n",
        "        return [\"__empty__\"]\n",
        "    first = toks[0][:4]\n",
        "    last = toks[-1][:4]\n",
        "    initials = \"\".join(t[0] for t in toks[:3])\n",
        "    return list(set([\n",
        "        f\"f:{first}\",\n",
        "        f\"l:{last}\",\n",
        "        f\"fl:{first}:{last}\",\n",
        "        f\"i:{initials}\",\n",
        "    ]))\n",
        "\n",
        "def _cosine(a: List[float], b: List[float]) -> float:\n",
        "    a = np.array(a, dtype=float); b = np.array(b, dtype=float)\n",
        "    na = np.linalg.norm(a); nb = np.linalg.norm(b)\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / (na * nb))\n",
        "\n",
        "def fetch_case_actors(case_id: str) -> pd.DataFrame:\n",
        "    q = \"\"\"\n",
        "    MATCH (a:Actor {case_id:$cid})\n",
        "    RETURN a.name AS name, a.type AS actor_type, a.role AS role,\n",
        "           a.description AS description, a.aliases AS aliases\n",
        "    \"\"\"\n",
        "    with driver.session() as s:\n",
        "        rows = [r.data() for r in s.run(q, cid=case_id)]\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df[\"name_norm\"] = df[\"name\"].apply(_norm_name)\n",
        "    df[\"block_keys\"] = df[\"name\"].apply(_blocking_keys)\n",
        "    return df\n",
        "\n",
        "def suggest_actor_duplicates(\n",
        "    case_id: str,\n",
        "    threshold: float = 88.0,\n",
        "    use_embeddings: bool = True,\n",
        "    max_pairs_per_block: int = 5000\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns candidate duplicate pairs with a composite score.\n",
        "\n",
        "    - Blocking: compare only within shared blocking keys (high recall).\n",
        "    - Scoring: RapidFuzz + optional cosine(name/role embeddings).\n",
        "    \"\"\"\n",
        "    df = fetch_case_actors(case_id)\n",
        "    if df.empty or len(df) < 2:\n",
        "        console.print(\"[yellow]‚ö†Ô∏è Not enough actors to deduplicate.[/yellow]\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Build inverted index for blocks ‚Üí row indices\n",
        "    block_map: Dict[str, List[int]] = {}\n",
        "    for idx, keys in enumerate(df[\"block_keys\"]):\n",
        "        for k in keys:\n",
        "            block_map.setdefault(k, []).append(idx)\n",
        "\n",
        "    # Optionally pre-embed (name + role) once\n",
        "    vecs = None\n",
        "    if use_embeddings and \"vector_engine\" in globals():\n",
        "        texts = (df[\"name\"].fillna(\"\") + \": \" + df[\"role\"].fillna(\"\")).tolist()\n",
        "        vecs = vector_engine.embed_batch(texts, task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "\n",
        "    seen = set()\n",
        "    out = []\n",
        "\n",
        "    for bk, idxs in block_map.items():\n",
        "        if len(idxs) < 2:\n",
        "            continue\n",
        "        # Safety cap to avoid accidental blow-ups\n",
        "        if len(idxs) * (len(idxs)-1) / 2 > max_pairs_per_block:\n",
        "            continue\n",
        "\n",
        "        for i_pos, i in enumerate(idxs):\n",
        "            for j in idxs[i_pos+1:]:\n",
        "                key = (min(i, j), max(i, j))\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "\n",
        "                n1 = df.at[i, \"name\"]\n",
        "                n2 = df.at[j, \"name\"]\n",
        "\n",
        "                f1 = fuzz.QRatio(_norm_name(n1), _norm_name(n2))\n",
        "                f2 = fuzz.token_sort_ratio(_norm_name(n1), _norm_name(n2))\n",
        "                f3 = fuzz.token_set_ratio(_norm_name(n1), _norm_name(n2))\n",
        "                fuzz_score = max(f1, f2, f3)\n",
        "\n",
        "                emb_score = None\n",
        "                if vecs is not None:\n",
        "                    emb_score = _cosine(vecs[i], vecs[j]) * 100.0\n",
        "\n",
        "                # Composite: mostly string, with semantic tie-breaker\n",
        "                if emb_score is None:\n",
        "                    score = fuzz_score\n",
        "                else:\n",
        "                    score = 0.70 * fuzz_score + 0.30 * emb_score\n",
        "\n",
        "                if score >= threshold:\n",
        "                    out.append({\n",
        "                        \"case_id\": case_id,\n",
        "                        \"block\": bk,\n",
        "                        \"name_1\": n1,\n",
        "                        \"name_2\": n2,\n",
        "                        \"fuzz_score\": round(fuzz_score, 2),\n",
        "                        \"emb_score\": round(emb_score, 2) if emb_score is not None else None,\n",
        "                        \"score\": round(score, 2),\n",
        "                        \"type_1\": df.at[i, \"actor_type\"],\n",
        "                        \"type_2\": df.at[j, \"actor_type\"],\n",
        "                    })\n",
        "\n",
        "    res = pd.DataFrame(out).sort_values([\"score\", \"fuzz_score\"], ascending=False)\n",
        "    console.print(f\"[green]‚úÖ Candidate pairs found: {len(res)}[/green]\")\n",
        "    return res\n",
        "\n",
        "def apply_soft_merges(case_id: str, merges: Dict[str, str]) -> None:\n",
        "    \"\"\"\n",
        "    Safe option: do NOT refactor/merge nodes.\n",
        "    Instead:\n",
        "      - create (dup)-[:SAME_AS]->(canonical)\n",
        "      - append dup name into canonical.aliases\n",
        "      - tag dup with canonical_name for easy filtering\n",
        "    \"\"\"\n",
        "    if not merges:\n",
        "        console.print(\"[yellow]‚ö†Ô∏è No merges supplied.[/yellow]\")\n",
        "        return\n",
        "\n",
        "    q = \"\"\"\n",
        "    MATCH (d:Actor {case_id:$cid, name:$dup})\n",
        "    MATCH (c:Actor {case_id:$cid, name:$canon})\n",
        "    MERGE (d)-[r:SAME_AS]->(c)\n",
        "    SET r.score = $score, d.canonical_name = $canon\n",
        "    SET c.aliases = apoc.coll.toSet(coalesce(c.aliases, []) + [$dup])\n",
        "    RETURN d.name AS duplicate, c.name AS canonical\n",
        "    \"\"\"\n",
        "    # APOC is optional; if not available, we fall back without alias set-dedup.\n",
        "    q_no_apoc = \"\"\"\n",
        "    MATCH (d:Actor {case_id:$cid, name:$dup})\n",
        "    MATCH (c:Actor {case_id:$cid, name:$canon})\n",
        "    MERGE (d)-[r:SAME_AS]->(c)\n",
        "    SET r.score = $score, d.canonical_name = $canon\n",
        "    SET c.aliases = coalesce(c.aliases, []) + [$dup]\n",
        "    RETURN d.name AS duplicate, c.name AS canonical\n",
        "    \"\"\"\n",
        "\n",
        "    with driver.session() as s:\n",
        "        for dup, canon in merges.items():\n",
        "            score = None\n",
        "            try:\n",
        "                s.run(q, cid=case_id, dup=dup, canon=canon, score=score)\n",
        "            except Exception:\n",
        "                s.run(q_no_apoc, cid=case_id, dup=dup, canon=canon, score=score)\n",
        "\n",
        "    console.print(\"[green]‚úÖ Soft merges applied (SAME_AS edges created).[/green]\")\n",
        "\n",
        "# Example:\n",
        "# case_id = PROCESS_QUEUE[0].case_id  # after upload\n",
        "# candidates = suggest_actor_duplicates(case_id, threshold=90.0)\n",
        "# candidates.head(20)\n",
        "\n",
        "# After manual review:\n",
        "# MERGES = {\"UN\": \"United Nations\", \"U.N.\": \"United Nations\"}\n",
        "# apply_soft_merges(case_id, MERGES)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxlmYBTl536c"
      },
      "source": [
        "---\n",
        "# üßæ BLOCK 6C: Evidence-first Ingest (Runs + Provenance + CTX write)\n",
        "**Goal:** A stronger substrate for conflict analysis:\n",
        "\n",
        "- Writes `SourceDoc/Chunk/Span` evidence nodes alongside CTX entities\n",
        "- Records an `ExtractionRun` with config snapshot\n",
        "- Produces span-level `EVIDENCE_FOR` links for Actors and Claims (easy to extend)\n",
        "\n",
        "Entry point: `upload_and_ingest_v7()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjZVnRon_Wvq"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 6C: EVIDENCE-FIRST INGEST ORCHESTRATOR\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def _read_uploaded_file(name: str, data: bytes) -> str:\n",
        "    for enc in (\"utf-8\",\"utf-16\",\"latin-1\"):\n",
        "        try:\n",
        "            return data.decode(enc)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return data.decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "async def _extract_all_chunks(doc: \"CaseDocument\", concurrency: int = 6) -> List[\"FullExtraction\"]:\n",
        "    sem = asyncio.Semaphore(concurrency)\n",
        "    outs: List[\"FullExtraction\"] = []\n",
        "\n",
        "    async def _one(ch: \"Chunk\"):\n",
        "        async with sem:\n",
        "            return await extractor.extract_chunk(ch.content)\n",
        "\n",
        "    tasks = [_one(ch) for ch in doc.chunks]\n",
        "    for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Extracting chunks\"):\n",
        "        outs.append(await fut)\n",
        "\n",
        "    return outs\n",
        "\n",
        "def _merge_extractions(exts: List[\"FullExtraction\"]) -> Tuple[\"FullExtraction\", Dict[str,str]]:\n",
        "    full = FullExtraction()\n",
        "    for e in exts:\n",
        "        full.actors += e.actors\n",
        "        full.relationships += e.relationships\n",
        "        full.events += e.events\n",
        "        full.claims += e.claims\n",
        "        full.interests += e.interests\n",
        "        full.leverage += e.leverage\n",
        "        full.constraints += e.constraints\n",
        "        full.commitments += e.commitments\n",
        "        full.narratives += e.narratives\n",
        "\n",
        "    merged_actors, mapping = resolver.merge(full.actors)\n",
        "    full.actors = merged_actors\n",
        "    full = resolver.apply_mapping(full, mapping)\n",
        "    return full, mapping\n",
        "\n",
        "def ingest_case_v7(case_name: str, filename: str, raw_text: str, concurrency: int = 6) -> Dict[str, Any]:\n",
        "    doc = process_upload(case_name, filename, raw_text)\n",
        "\n",
        "    rid = run_id()\n",
        "    cfg_snapshot = {\n",
        "        \"extraction_model\": config.extraction_model,\n",
        "        \"embedding_model\": config.embedding_model,\n",
        "        \"chunk_size\": config.chunk_size,\n",
        "        \"chunk_overlap\": config.chunk_overlap,\n",
        "        \"er_threshold\": config.er_threshold,\n",
        "        \"onto_rag_active\": bool(getattr(onto_rag, \"is_initialized\", False)),\n",
        "    }\n",
        "\n",
        "    write_source_doc_and_chunks(driver, doc)\n",
        "    write_extraction_run(driver, rid, doc.case_id, cfg_snapshot)\n",
        "    attach_run_source(driver, rid, doc)\n",
        "\n",
        "    exts = asyncio.run(_extract_all_chunks(doc, concurrency=concurrency))\n",
        "    full_ext, mapping = _merge_extractions(exts)\n",
        "\n",
        "    stats = writer.write_extraction(doc.case_id, full_ext)\n",
        "    span_count = write_spans_and_evidence(driver, doc, full_ext, rid)\n",
        "\n",
        "    return {\n",
        "        \"case_id\": doc.case_id,\n",
        "        \"run_id\": rid,\n",
        "        \"writer_stats\": stats,\n",
        "        \"span_count\": span_count,\n",
        "        \"name_mapping\": mapping,\n",
        "    }\n",
        "\n",
        "def upload_and_ingest_v7(case_name: str = \"TACITUS_CASE\", concurrency: int = 6) -> List[Dict[str, Any]]:\n",
        "    uploaded = files.upload()\n",
        "    results = []\n",
        "    for fname, data in uploaded.items():\n",
        "        raw = _read_uploaded_file(fname, data)\n",
        "        console.print(f\"[cyan]üìÑ Ingesting: {fname}[/cyan]\")\n",
        "        res = ingest_case_v7(case_name=case_name, filename=fname, raw_text=raw, concurrency=concurrency)\n",
        "        console.print(f\"[green]‚úÖ Done: case_id={res['case_id']} run_id={res['run_id']} spans={res['span_count']}[/green]\")\n",
        "        results.append(res)\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Evidence-first ingest ready. Next: run `upload_and_ingest_v7('My Case Name')`.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHMo1fX4g3fh"
      },
      "source": [
        "---\n",
        "# ‚úÖ BLOCK 6D: QA Gates + Run Metrics (Operational Trace)\n",
        "**Goal:** Treat extraction like a production pipeline:\n",
        "\n",
        "- Check basic schema health (suspicious emptiness, missing actors)\n",
        "- Check graph integrity (orphan nodes)\n",
        "- Persist QA + counts back onto the `ExtractionRun` node\n",
        "\n",
        "Run after `upload_and_ingest_v7()` returns `case_id` + `run_id`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avhK2Pg7zHer"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 6D: QUALITY GATES + RUN METRICS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def qa_graph(case_id: str) -> Dict[str, Any]:\n",
        "    with driver.session() as s:\n",
        "        by_label = {}\n",
        "        for r in s.run(\"MATCH (n {case_id:$cid}) RETURN labels(n)[0] AS label, count(*) AS n\", cid=case_id):\n",
        "            by_label[r[\"label\"]] = r[\"n\"]\n",
        "\n",
        "        orphan = s.run(\"\"\"\n",
        "            MATCH (n {case_id:$cid})\n",
        "            WHERE NOT (n)--()\n",
        "            RETURN count(n) AS c\n",
        "        \"\"\", cid=case_id).single()[\"c\"]\n",
        "\n",
        "    return {\"by_label\": by_label, \"orphans\": int(orphan)}\n",
        "\n",
        "def persist_run_metrics(run_id: str, payload: Dict[str, Any]) -> None:\n",
        "    with driver.session() as s:\n",
        "        s.run(\"\"\"\n",
        "            MATCH (r:ExtractionRun {run_id:$rid})\n",
        "            SET r.finished_at=$ts,\n",
        "                r.metrics=$m,\n",
        "                r.qa_issues=$issues\n",
        "        \"\"\", rid=run_id,\n",
        "             ts=datetime.now(timezone.utc).isoformat(),\n",
        "             m=json.dumps(payload, default=str),\n",
        "             issues=json.dumps(payload.get(\"issues\", []), default=str))\n",
        "\n",
        "def run_qa_and_persist(case_id: str, run_id: str) -> Dict[str, Any]:\n",
        "    graph_stats = qa_graph(case_id)\n",
        "\n",
        "    issues = []\n",
        "    if graph_stats[\"by_label\"].get(\"Actor\", 0) == 0:\n",
        "        issues.append(\"No Actor nodes in graph.\")\n",
        "    if graph_stats[\"by_label\"].get(\"Claim\", 0) == 0 and graph_stats[\"by_label\"].get(\"Event\", 0) == 0:\n",
        "        issues.append(\"No Claim/Event nodes in graph.\")\n",
        "\n",
        "    payload = {\n",
        "        \"case_id\": case_id,\n",
        "        \"run_id\": run_id,\n",
        "        \"graph_stats\": graph_stats,\n",
        "        \"issues\": issues,\n",
        "    }\n",
        "    persist_run_metrics(run_id, payload)\n",
        "    print(f\"‚úÖ QA persisted. Orphans={graph_stats['orphans']} Issues={len(issues)}\")\n",
        "    return payload\n",
        "\n",
        "print(\"‚úÖ QA helpers ready. Typical use:\")\n",
        "print(\"   payload = run_qa_and_persist(case_id, run_id)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zvSyIRS3kuH"
      },
      "source": [
        "---\n",
        "# üß† BLOCK 6E: Build a Reasoning Graph (Issues, Contradictions, Influence)\n",
        "**Goal:** Move from ‚Äúextracted facts‚Äù ‚Üí ‚Äústructured analysis substrate‚Äù.\n",
        "\n",
        "Adds derived nodes/edges:\n",
        "- `Issue` clusters (from Claim embeddings)\n",
        "- `POTENTIAL_CONTRADICTION` edges (simple heuristic)\n",
        "- `influence_score` on Actors (PageRank over Actor‚ÜîActor edges)\n",
        "\n",
        "Deterministic today; swappable later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8vUX1V-PVla"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 6E: REASONING GRAPH BUILDERS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import networkx as nx\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def _cosine_np(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
        "    return A @ B.T\n",
        "\n",
        "def _top_terms(texts: List[str], k: int = 6) -> List[str]:\n",
        "    if not texts:\n",
        "        return []\n",
        "    vec = TfidfVectorizer(stop_words=\"english\", max_features=3000)\n",
        "    X = vec.fit_transform(texts)\n",
        "    scores = np.asarray(X.sum(axis=0)).ravel()\n",
        "    terms = np.array(vec.get_feature_names_out())\n",
        "    idx = scores.argsort()[::-1][:k]\n",
        "    return terms[idx].tolist()\n",
        "\n",
        "def build_issue_graph(case_id: str, distance_threshold: float = 0.35, min_cluster_size: int = 2) -> Dict[str, Any]:\n",
        "    with driver.session() as s:\n",
        "        rows = [dict(r) for r in s.run(\"MATCH (c:Claim {case_id:$cid}) RETURN c.statement AS statement, c.embedding AS emb\", cid=case_id)]\n",
        "\n",
        "    rows = [r for r in rows if r.get(\"emb\") and isinstance(r[\"emb\"], list)]\n",
        "    if len(rows) < 2:\n",
        "        print(\"‚ö†Ô∏è Not enough embedded claims to build issues.\")\n",
        "        return {\"issues_created\": 0}\n",
        "\n",
        "    texts = [r[\"statement\"] for r in rows]\n",
        "    E = np.array([r[\"emb\"] for r in rows], dtype=float)\n",
        "\n",
        "    clustering = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        distance_threshold=distance_threshold,\n",
        "        metric=\"cosine\",\n",
        "        linkage=\"average\"\n",
        "    )\n",
        "    labels = clustering.fit_predict(E)\n",
        "\n",
        "    clusters: Dict[int, List[int]] = defaultdict(list)\n",
        "    for i, lab in enumerate(labels):\n",
        "        clusters[int(lab)].append(i)\n",
        "\n",
        "    created = 0\n",
        "    with driver.session() as s:\n",
        "        for lab, idxs in clusters.items():\n",
        "            if len(idxs) < min_cluster_size:\n",
        "                continue\n",
        "            stmts = [texts[i] for i in idxs]\n",
        "            issue_terms = _top_terms(stmts, k=6)\n",
        "            issue_label = \"; \".join(issue_terms[:4]) if issue_terms else f\"issue_{lab}\"\n",
        "            iid = mk_id(\"RZN:ISS_\", case_id, str(lab), issue_label, n=14)\n",
        "\n",
        "            s.run(\"\"\"\n",
        "                MERGE (i:Issue {issue_id:$iid})\n",
        "                SET i.case_id=$cid,\n",
        "                    i.issue_label=$lbl,\n",
        "                    i.top_terms=$terms,\n",
        "                    i.size=$sz,\n",
        "                    i.graph_category='RZN',\n",
        "                    i.created_at=$ts\n",
        "            \"\"\", iid=iid, cid=case_id, lbl=issue_label, terms=issue_terms, sz=len(idxs),\n",
        "                 ts=datetime.now(timezone.utc).isoformat())\n",
        "\n",
        "            for stmt in stmts:\n",
        "                s.run(\"\"\"\n",
        "                    MATCH (c:Claim {case_id:$cid, statement:$stmt})\n",
        "                    MATCH (i:Issue {issue_id:$iid})\n",
        "                    MERGE (c)-[:ABOUT_ISSUE]->(i)\n",
        "                \"\"\", cid=case_id, stmt=stmt, iid=iid)\n",
        "\n",
        "            created += 1\n",
        "\n",
        "    print(f\"‚úÖ Issues created: {created}\")\n",
        "    return {\"issues_created\": created}\n",
        "\n",
        "_NEG_WORDS = {\"not\",\"never\",\"no\",\"none\",\"cannot\",\"can't\",\"won't\",\"without\",\"deny\",\"denies\",\"denied\"}\n",
        "\n",
        "def detect_contradictions(case_id: str, min_similarity: float = 0.72) -> int:\n",
        "    created = 0\n",
        "    with driver.session() as s:\n",
        "        issues = [r[\"issue_id\"] for r in s.run(\"MATCH (i:Issue {case_id:$cid}) RETURN i.issue_id AS issue_id\", cid=case_id)]\n",
        "        for iid in issues:\n",
        "            claims = [dict(r) for r in s.run(\"\"\"\n",
        "                MATCH (c:Claim {case_id:$cid})-[:ABOUT_ISSUE]->(i:Issue {issue_id:$iid})\n",
        "                RETURN c.statement AS statement, c.embedding AS emb\n",
        "            \"\"\", cid=case_id, iid=iid)]\n",
        "            claims = [c for c in claims if c.get(\"emb\") and isinstance(c[\"emb\"], list)]\n",
        "            if len(claims) < 2:\n",
        "                continue\n",
        "\n",
        "            texts = [c[\"statement\"] for c in claims]\n",
        "            E = np.array([c[\"emb\"] for c in claims], dtype=float)\n",
        "            S = _cosine_np(E, E)\n",
        "\n",
        "            for i in range(len(texts)):\n",
        "                for j in range(i+1, len(texts)):\n",
        "                    sim = float(S[i,j])\n",
        "                    if sim < min_similarity:\n",
        "                        continue\n",
        "                    ti = set(re.findall(r\"[a-z']+\", texts[i].lower()))\n",
        "                    tj = set(re.findall(r\"[a-z']+\", texts[j].lower()))\n",
        "                    ni = len(ti & _NEG_WORDS) > 0\n",
        "                    nj = len(tj & _NEG_WORDS) > 0\n",
        "                    if ni ^ nj:\n",
        "                        s.run(\"\"\"\n",
        "                            MATCH (a:Claim {case_id:$cid, statement:$s1})\n",
        "                            MATCH (b:Claim {case_id:$cid, statement:$s2})\n",
        "                            MERGE (a)-[r:POTENTIAL_CONTRADICTION]->(b)\n",
        "                            SET r.issue_id=$iid,\n",
        "                                r.similarity=$sim,\n",
        "                                r.rationale='negation_mismatch',\n",
        "                                r.graph_category='RZN'\n",
        "                        \"\"\", cid=case_id, s1=texts[i], s2=texts[j], iid=iid, sim=sim)\n",
        "                        created += 1\n",
        "    print(f\"‚úÖ Contradiction edges created: {created}\")\n",
        "    return created\n",
        "\n",
        "def compute_actor_influence(case_id: str) -> Dict[str, float]:\n",
        "    G = nx.DiGraph()\n",
        "    with driver.session() as s:\n",
        "        rows = s.run(\"\"\"\n",
        "            MATCH (a:Actor {case_id:$cid})-[r]->(b:Actor {case_id:$cid})\n",
        "            RETURN a.name AS src, b.name AS dst\n",
        "        \"\"\", cid=case_id).data()\n",
        "\n",
        "    for r in rows:\n",
        "        G.add_edge(r[\"src\"], r[\"dst\"])\n",
        "\n",
        "    if G.number_of_nodes() == 0:\n",
        "        print(\"‚ö†Ô∏è No Actor-Actor edges found; influence scoring skipped.\")\n",
        "        return {}\n",
        "\n",
        "    pr = nx.pagerank(G, alpha=0.85)\n",
        "    with driver.session() as s:\n",
        "        for name, score in pr.items():\n",
        "            s.run(\"MATCH (a:Actor {case_id:$cid, name:$name}) SET a.influence_score=$sc\", cid=case_id, name=name, sc=float(score))\n",
        "\n",
        "    print(f\"‚úÖ Wrote influence_score for {len(pr)} actors.\")\n",
        "    return pr\n",
        "\n",
        "print(\"‚úÖ Reasoning builders ready. Typical flow:\")\n",
        "print(\"   build_issue_graph(case_id)\")\n",
        "print(\"   detect_contradictions(case_id)\")\n",
        "print(\"   compute_actor_influence(case_id)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZs7sS0sxhEg"
      },
      "source": [
        "---\n",
        "# üîç BLOCK 7: Semantic Search\n",
        "**Search your knowledge graph using natural language.**\n",
        "\n",
        "Examples:\n",
        "- \"actors with high leverage\"\n",
        "- \"escalation events\"\n",
        "- \"demands and threats\"\n",
        "- \"security concerns\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw3GACrDxhEg"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 7: SEMANTIC SEARCH\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ PURPOSE: Search the graph using natural language queries\n",
        "# ‚è±Ô∏è TIME: Instant per query\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class SemanticSearch:\n",
        "    \"\"\"Natural language search across the knowledge graph.\"\"\"\n",
        "\n",
        "    def __init__(self, drv, vec_engine: VectorEngine):\n",
        "        self.driver = drv\n",
        "        self.vector_engine = vec_engine\n",
        "\n",
        "    def search(self, query: str, node_labels: List[str] = None,\n",
        "               case_id: str = None, top_k: int = 15) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Semantic search across graph nodes.\n",
        "\n",
        "        Args:\n",
        "            query: Natural language query\n",
        "            node_labels: Optional filter (e.g., ['Actor', 'Event'])\n",
        "            case_id: Optional filter to specific case\n",
        "            top_k: Number of results\n",
        "        \"\"\"\n",
        "        # Generate query embedding\n",
        "        query_vec = self.vector_engine.embed_text(query, \"RETRIEVAL_QUERY\")\n",
        "\n",
        "        # Build Cypher\n",
        "        label_filter = \"\"\n",
        "        if node_labels:\n",
        "            label_parts = [f\"'{l}' IN labels(n)\" for l in node_labels]\n",
        "            label_filter = f\" AND ({' OR '.join(label_parts)})\"\n",
        "\n",
        "        case_filter = f\" AND n.case_id = '{case_id}'\" if case_id else \"\"\n",
        "\n",
        "        cypher = f\"\"\"\n",
        "            MATCH (n)\n",
        "            WHERE n.embedding IS NOT NULL {label_filter} {case_filter}\n",
        "            WITH n, vector.similarity.cosine(n.embedding, $query_vec) AS score\n",
        "            WHERE score > 0.4\n",
        "            ORDER BY score DESC\n",
        "            LIMIT $top_k\n",
        "            RETURN\n",
        "                labels(n)[0] AS Type,\n",
        "                COALESCE(n.name, LEFT(n.description, 80), LEFT(n.statement, 80)) AS Content,\n",
        "                n.case_id AS Case,\n",
        "                round(score * 1000) / 1000 AS Score\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            with self.driver.session() as s:\n",
        "                result = s.run(cypher, query_vec=query_vec, top_k=top_k)\n",
        "                return pd.DataFrame([dict(r) for r in result])\n",
        "        except Exception as e:\n",
        "            console.print(f\"[yellow]‚ö†Ô∏è Search error: {e}[/yellow]\")\n",
        "            console.print(\"[yellow]Tip: Make sure you have nodes with embeddings.[/yellow]\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def find_similar(self, entity_name: str, entity_type: str = \"Actor\") -> pd.DataFrame:\n",
        "        \"\"\"Find entities similar to a given one.\"\"\"\n",
        "        with self.driver.session() as s:\n",
        "            # Get source embedding\n",
        "            result = s.run(f\"\"\"\n",
        "                MATCH (n:{entity_type} {{name: $name}})\n",
        "                WHERE n.embedding IS NOT NULL\n",
        "                RETURN n.embedding AS emb, n.case_id AS source_case\n",
        "                LIMIT 1\n",
        "            \"\"\", name=entity_name).single()\n",
        "\n",
        "            if not result:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            source_emb = result['emb']\n",
        "            source_case = result['source_case']\n",
        "\n",
        "            # Find similar\n",
        "            similar = s.run(f\"\"\"\n",
        "                MATCH (n:{entity_type})\n",
        "                WHERE n.embedding IS NOT NULL AND n.name <> $name\n",
        "                WITH n, vector.similarity.cosine(n.embedding, $emb) AS score\n",
        "                WHERE score > 0.5\n",
        "                ORDER BY score DESC\n",
        "                LIMIT 10\n",
        "                RETURN n.name AS Name, n.case_id AS Case, round(score * 1000) / 1000 AS Similarity\n",
        "            \"\"\", name=entity_name, emb=source_emb)\n",
        "\n",
        "            return pd.DataFrame([dict(r) for r in similar])\n",
        "\n",
        "# Initialize\n",
        "semantic_search = SemanticSearch(driver, vector_engine)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERACTIVE UI\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def get_cases_for_dropdown():\n",
        "    \"\"\"Get list of cases for dropdown.\"\"\"\n",
        "    with driver.session() as s:\n",
        "        result = s.run(\"MATCH (c:Case) RETURN c.case_name AS name, c.case_id AS id ORDER BY c.created_at DESC\")\n",
        "        return [(r['name'], r['id']) for r in result]\n",
        "\n",
        "# Widgets\n",
        "w_query = widgets.Text(\n",
        "    placeholder='e.g., \"high leverage actors\" or \"escalation events\"',\n",
        "    description='Query:',\n",
        "    layout={'width': '500px'}\n",
        ")\n",
        "\n",
        "w_labels = widgets.SelectMultiple(\n",
        "    options=['Actor', 'Event', 'Claim', 'Interest', 'Leverage', 'Constraint', 'Narrative', 'Commitment', 'Theory', 'Concept'],\n",
        "    value=[],\n",
        "    description='Filter:',\n",
        "    layout={'width': '150px', 'height': '120px'}\n",
        ")\n",
        "\n",
        "cases = get_cases_for_dropdown()\n",
        "w_case = widgets.Dropdown(\n",
        "    options=[('All Cases', None)] + cases,\n",
        "    description='Case:',\n",
        "    layout={'width': '300px'}\n",
        ")\n",
        "\n",
        "btn_search = widgets.Button(description='üîç Search', button_style='primary', layout={'width': '100px'})\n",
        "out_search = widgets.Output()\n",
        "\n",
        "def on_search(_):\n",
        "    with out_search:\n",
        "        clear_output()\n",
        "        if not w_query.value:\n",
        "            print(\"Enter a search query.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üîç Searching: '{w_query.value}'...\")\n",
        "\n",
        "        labels = list(w_labels.value) if w_labels.value else None\n",
        "        df = semantic_search.search(\n",
        "            query=w_query.value,\n",
        "            node_labels=labels,\n",
        "            case_id=w_case.value,\n",
        "            top_k=20\n",
        "        )\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"\\n‚ö†Ô∏è No results. Try:\")\n",
        "            print(\"   ‚Ä¢ Different keywords\")\n",
        "            print(\"   ‚Ä¢ Remove filters\")\n",
        "            print(\"   ‚Ä¢ Make sure documents are processed with embeddings\")\n",
        "        else:\n",
        "            print(f\"\\n‚úÖ Found {len(df)} results:\\n\")\n",
        "            display(df.style.hide(axis='index').background_gradient(subset=['Score'], cmap='Greens'))\n",
        "\n",
        "btn_search.on_click(on_search)\n",
        "\n",
        "# Also search on Enter key\n",
        "w_query.on_submit(on_search)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>üß† Semantic Search</h3>\"),\n",
        "    widgets.HBox([w_query, btn_search]),\n",
        "    widgets.HBox([w_case, w_labels]),\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    out_search\n",
        "]))\n",
        "\n",
        "print(\"\\nüí° Tips:\")\n",
        "print(\"   ‚Ä¢ Search finds nodes by meaning, not just keywords\")\n",
        "print(\"   ‚Ä¢ Select multiple types in Filter to narrow results\")\n",
        "print(\"   ‚Ä¢ Press Enter or click Search to query\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NIR9WduxhEh"
      },
      "source": [
        "---\n",
        "# üìä BLOCK 8: Cypher Query Lab\n",
        "**Run predefined analytics queries on your conflict graph.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YoGShEJxhEi"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 8: CYPHER QUERY LAB\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "QUERY_LIBRARY = {\n",
        "    \"üë• All Actors\": \"MATCH (a:Actor {case_id: $cid}) RETURN a.name AS Actor, a.role AS Role, a.type AS Type, a.is_primary AS Primary ORDER BY a.is_primary DESC\",\n",
        "    \"‚öîÔ∏è Conflicts (OPPOSES)\": \"MATCH (a:Actor {case_id: $cid})-[r:OPPOSES]->(b:Actor) RETURN a.name AS Actor1, 'OPPOSES' AS Rel, b.name AS Actor2\",\n",
        "    \"ü§ù Alliances\": \"MATCH (a:Actor {case_id: $cid})-[r:ALLIES_WITH]->(b:Actor) RETURN a.name AS Actor1, 'ALLIES_WITH' AS Rel, b.name AS Actor2\",\n",
        "    \"üí™ Leverage\": \"MATCH (a:Actor {case_id: $cid})-[:HOLDS_LEVERAGE]->(l:Leverage) RETURN a.name AS Actor, l.type AS Type, l.description AS Leverage\",\n",
        "    \"üéØ Interests\": \"MATCH (a:Actor {case_id: $cid})-[:HAS_INTEREST]->(i:Interest) RETURN a.name AS Actor, i.type AS Type, i.description AS Interest\",\n",
        "    \"üìÖ Events\": \"MATCH (e:Event {case_id: $cid}) RETURN e.date AS Date, e.description AS Event, e.impact AS Impact ORDER BY e.date\",\n",
        "    \"üó£Ô∏è Claims\": \"MATCH (a:Actor {case_id: $cid})-[:MAKES_CLAIM]->(c:Claim) RETURN a.name AS Actor, c.type AS Type, c.statement AS Claim\",\n",
        "    \"üìñ Narratives\": \"MATCH (a:Actor {case_id: $cid})-[:PROMOTES_NARRATIVE]->(n:Narrative) RETURN a.name AS Actor, n.frame AS Frame, n.description AS Narrative\",\n",
        "    \"üï∏Ô∏è Most Connected\": \"MATCH (a:Actor {case_id: $cid}) OPTIONAL MATCH (a)-[r]-() WITH a, count(r) AS c RETURN a.name AS Actor, c AS Connections ORDER BY c DESC LIMIT 10\",\n",
        "    \"üìà Statistics\": \"MATCH (n {case_id: $cid}) WITH labels(n)[0] AS type, count(*) AS c RETURN type AS Type, c AS Count ORDER BY c DESC\"\n",
        "}\n",
        "\n",
        "def run_query(query, case_id):\n",
        "    with driver.session() as s:\n",
        "        return pd.DataFrame([dict(r) for r in s.run(query, cid=case_id)])\n",
        "\n",
        "# UI\n",
        "cases = get_cases_for_dropdown()\n",
        "if cases:\n",
        "    w_case_q = widgets.Dropdown(options=cases, description='Case:', layout={'width': '350px'})\n",
        "    w_query_type = widgets.Dropdown(options=list(QUERY_LIBRARY.keys()), description='Query:')\n",
        "    out_query = widgets.Output()\n",
        "\n",
        "    def update_query(_):\n",
        "        with out_query:\n",
        "            clear_output()\n",
        "            df = run_query(QUERY_LIBRARY[w_query_type.value], w_case_q.value)\n",
        "            if df.empty:\n",
        "                print(\"‚ö†Ô∏è No results\")\n",
        "            else:\n",
        "                print(f\"‚úÖ {len(df)} results:\")\n",
        "                display(df.style.hide(axis='index'))\n",
        "\n",
        "    w_case_q.observe(update_query, names='value')\n",
        "    w_query_type.observe(update_query, names='value')\n",
        "    display(widgets.VBox([widgets.HTML('<h3>üìä Query Lab</h3>'), widgets.HBox([w_case_q, w_query_type]), out_query]))\n",
        "    update_query(None)\n",
        "else:\n",
        "    print('‚ö†Ô∏è No cases found. Process documents first.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Q-O9F_xhEi"
      },
      "source": [
        "---\n",
        "# üï∏Ô∏è BLOCK 9: Visualization Dashboard\n",
        "**Interactive network and timeline visualizations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XMv15AqxhEj"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 9: VISUALIZATION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "from pyvis.network import Network\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def visualize_graph(case_id, height='550px'):\n",
        "    print(f'üï∏Ô∏è Building graph...')\n",
        "    net = Network(height=height, width='100%', bgcolor='#1a1a2e', font_color='white', notebook=True, cdn_resources='remote')\n",
        "    net.barnes_hut(gravity=-2500, spring_length=100)\n",
        "\n",
        "    added = set()\n",
        "    with driver.session() as s:\n",
        "        # Actors\n",
        "        for a in s.run('MATCH (a:Actor {case_id: $cid}) RETURN a.name AS n, a.is_primary AS p, a.role AS r', cid=case_id):\n",
        "            net.add_node(a['n'], label=a['n'], color='#e74c3c' if a['p'] else '#c0392b', size=30 if a['p'] else 20, title=a['r'])\n",
        "            added.add(a['n'])\n",
        "\n",
        "        # Other nodes\n",
        "        colors = {'Event': '#9b59b6', 'Claim': '#3498db', 'Interest': '#2ecc71', 'Leverage': '#f39c12'}\n",
        "        for o in s.run('MATCH (a:Actor {case_id: $cid})-[]->(n) WHERE n.case_id = $cid AND NOT n:Actor RETURN DISTINCT n.entity_id AS id, labels(n)[0] AS t, LEFT(COALESCE(n.description, n.statement), 25) AS l', cid=case_id):\n",
        "            if o['id'] not in added:\n",
        "                net.add_node(o['id'], label=o['l'] or o['t'], color=colors.get(o['t'], '#95a5a6'), size=12)\n",
        "                added.add(o['id'])\n",
        "\n",
        "        # Edges\n",
        "        edge_colors = {'OPPOSES': '#e74c3c', 'ALLIES_WITH': '#2ecc71'}\n",
        "        for r in s.run('MATCH (a {case_id: $cid})-[r]->(b) WHERE b.case_id = $cid OR b.case_id IS NULL RETURN COALESCE(a.name, a.entity_id) AS s, COALESCE(b.name, b.entity_id) AS t, type(r) AS rel', cid=case_id):\n",
        "            if r['s'] in added and r['t'] in added:\n",
        "                net.add_edge(r['s'], r['t'], color=edge_colors.get(r['rel'], '#555'), title=r['rel'])\n",
        "\n",
        "    print(f'‚úÖ {len(added)} nodes')\n",
        "    display(HTML(net.generate_html()))\n",
        "\n",
        "def visualize_timeline(case_id):\n",
        "    df = run_query('MATCH (e:Event {case_id: $cid}) RETURN e.date AS d, e.description AS desc, e.impact AS imp ORDER BY e.date', case_id)\n",
        "    if df.empty: return print('‚ö†Ô∏è No events')\n",
        "    colors = {'critical': '#e74c3c', 'high': '#f39c12', 'moderate': '#3498db'}\n",
        "    fig = go.Figure(go.Scatter(x=df['d'], y=[1]*len(df), mode='markers+text', marker=dict(size=12, color=[colors.get(str(x),'#95a5a6') for x in df['imp']]), text=[str(d)[:18]+'...' for d in df['desc']], textposition='top center', hovertext=df['desc']))\n",
        "    fig.update_layout(title='Timeline', height=220, template='plotly_dark', showlegend=False, yaxis={'visible': False})\n",
        "    fig.show()\n",
        "\n",
        "# UI\n",
        "cases = get_cases_for_dropdown()\n",
        "if cases:\n",
        "    w_case_v = widgets.Dropdown(options=cases, description='Case:', layout={'width': '350px'})\n",
        "    btn_g = widgets.Button(description='üï∏Ô∏è Graph', button_style='primary')\n",
        "    btn_t = widgets.Button(description='üìÖ Timeline', button_style='info')\n",
        "    out_v = widgets.Output()\n",
        "    btn_g.on_click(lambda _: (out_v.clear_output(), visualize_graph(w_case_v.value)) if True else None)\n",
        "    btn_t.on_click(lambda _: (out_v.clear_output(), visualize_timeline(w_case_v.value)) if True else None)\n",
        "\n",
        "    def on_graph(_):\n",
        "        with out_v:\n",
        "            clear_output()\n",
        "            visualize_graph(w_case_v.value)\n",
        "\n",
        "    def on_timeline(_):\n",
        "        with out_v:\n",
        "            clear_output()\n",
        "            visualize_timeline(w_case_v.value)\n",
        "\n",
        "    btn_g.on_click(on_graph)\n",
        "    btn_t.on_click(on_timeline)\n",
        "    display(widgets.VBox([widgets.HTML('<h3>üï∏Ô∏è Visualization</h3>'), widgets.HBox([w_case_v, btn_g, btn_t]), out_v]))\n",
        "else:\n",
        "    print('‚ö†Ô∏è No cases found.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEpouXurxhEj"
      },
      "source": [
        "---\n",
        "# üì§ BLOCK 10: Export & Database Manager\n",
        "**Export data and manage your graph database.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNzTspeFxhEl"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 10: EXPORT & DATABASE MANAGER\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class SPOExporter:\n",
        "    def __init__(self, drv): self.driver = drv\n",
        "\n",
        "    def export_triples(self, case_id, fmt='json'):\n",
        "        with self.driver.session() as s:\n",
        "            triples = [dict(r) for r in s.run('MATCH (a {case_id: $cid})-[r]->(b) RETURN COALESCE(a.name, a.entity_id) AS subject, type(r) AS predicate, COALESCE(b.name, b.description, b.entity_id) AS object, labels(a)[0] AS subj_type, labels(b)[0] AS obj_type', cid=case_id)]\n",
        "        if fmt == 'csv':\n",
        "            import io, csv\n",
        "            out = io.StringIO()\n",
        "            if triples:\n",
        "                w = csv.DictWriter(out, fieldnames=triples[0].keys())\n",
        "                w.writeheader()\n",
        "                w.writerows(triples)\n",
        "            return out.getvalue()\n",
        "        return json.dumps(triples, indent=2, default=str)\n",
        "\n",
        "    def download(self, case_id):\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M')\n",
        "        for fmt, ext in [('json', 'json'), ('csv', 'csv')]:\n",
        "            fn = f'tacitus_{case_id[:20]}_{ts}.{ext}'\n",
        "            with open(fn, 'w') as f: f.write(self.export_triples(case_id, fmt))\n",
        "            files.download(fn)\n",
        "            print(f'‚úÖ Downloaded: {fn}')\n",
        "\n",
        "class DBManager:\n",
        "    def __init__(self, drv): self.driver = drv\n",
        "\n",
        "    def get_stats(self):\n",
        "        with self.driver.session() as s:\n",
        "            cases = pd.DataFrame([dict(r) for r in s.run('MATCH (c:Case) OPTIONAL MATCH (c)-[:CONTAINS]->(n) WITH c, count(n) AS nodes RETURN c.case_name AS Name, c.case_id AS ID, nodes AS Nodes ORDER BY c.created_at DESC')])\n",
        "            types = pd.DataFrame([dict(r) for r in s.run('MATCH (n) RETURN labels(n)[0] AS Type, count(*) AS Count ORDER BY Count DESC')])\n",
        "        return cases, types\n",
        "\n",
        "    def delete_case(self, case_id):\n",
        "        with self.driver.session() as s:\n",
        "            s.run('MATCH (n {case_id: $cid}) DETACH DELETE n', cid=case_id)\n",
        "            s.run('MATCH (c:Case {case_id: $cid}) DELETE c', cid=case_id)\n",
        "\n",
        "    def wipe_all(self):\n",
        "        with self.driver.session() as s: s.run('MATCH (n) DETACH DELETE n')\n",
        "\n",
        "exporter = SPOExporter(driver)\n",
        "db_mgr = DBManager(driver)\n",
        "\n",
        "# UI\n",
        "out_db = widgets.Output()\n",
        "\n",
        "def refresh():\n",
        "    with out_db:\n",
        "        clear_output()\n",
        "        cases_df, types_df = db_mgr.get_stats()\n",
        "        print('üìÅ CASES:')\n",
        "        display(cases_df.style.hide(axis='index') if not cases_df.empty else print('   (none)'))\n",
        "        print('\\nüìä TYPES:')\n",
        "        display(types_df.style.hide(axis='index') if not types_df.empty else print('   (none)'))\n",
        "    w_del.options = get_cases_for_dropdown() or [('None', '')]\n",
        "\n",
        "btn_ref = widgets.Button(description='üîÑ Refresh', button_style='info')\n",
        "w_del = widgets.Dropdown(options=get_cases_for_dropdown() or [('None', '')], description='Case:')\n",
        "w_confirm = widgets.Checkbox(description='Confirm', value=False)\n",
        "btn_del = widgets.Button(description='üóëÔ∏è Delete', button_style='danger')\n",
        "btn_exp = widgets.Button(description='üì• Export', button_style='success')\n",
        "btn_wipe = widgets.Button(description='üí£ WIPE ALL', button_style='danger')\n",
        "\n",
        "btn_ref.on_click(lambda _: refresh())\n",
        "\n",
        "def on_del(_):\n",
        "    with out_db:\n",
        "        if not w_confirm.value: return print('‚ö†Ô∏è Check Confirm first')\n",
        "        if not w_del.value: return print('‚ö†Ô∏è Select case')\n",
        "        db_mgr.delete_case(w_del.value)\n",
        "        print(f'‚úÖ Deleted {w_del.value}')\n",
        "        w_confirm.value = False\n",
        "        refresh()\n",
        "\n",
        "def on_exp(_):\n",
        "    with out_db:\n",
        "        if w_del.value: exporter.download(w_del.value)\n",
        "\n",
        "def on_wipe(_):\n",
        "    with out_db:\n",
        "        if not w_confirm.value: return print('‚ö†Ô∏è Check Confirm first')\n",
        "        db_mgr.wipe_all()\n",
        "        print('‚úÖ Database wiped')\n",
        "        w_confirm.value = False\n",
        "        refresh()\n",
        "\n",
        "btn_del.on_click(on_del)\n",
        "btn_exp.on_click(on_exp)\n",
        "btn_wipe.on_click(on_wipe)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML('<h3>üóÑÔ∏è Database Manager</h3>'),\n",
        "    btn_ref, out_db,\n",
        "    widgets.HTML('<hr>'),\n",
        "    widgets.HBox([w_del, btn_exp, btn_del]),\n",
        "    widgets.HBox([w_confirm, btn_wipe])\n",
        "]))\n",
        "refresh()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W8ZHRVoQGb1"
      },
      "source": [
        "---\n",
        "# üì¶ BLOCK 10A: Case Bundle Export (Graph + Evidence + Run Traces)\n",
        "**Goal:** Export a complete case bundle for review, auditing, sharing, or offline analysis.\n",
        "\n",
        "Outputs:\n",
        "- triples.json / triples.csv\n",
        "- case_bundle.json (nodes + edges + evidence spans + runs)\n",
        "- zipped bundle you can download from Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2v9vYFD1i3r"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 10A: CASE BUNDLE EXPORT\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import zipfile\n",
        "\n",
        "def export_case_bundle(case_id: str, out_dir: str = \"tacitus_bundle\") -> str:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 1) triples via existing exporter\n",
        "    exporter = SPOExporter(driver)\n",
        "    triples_json = exporter.export_triples(case_id, fmt=\"json\")\n",
        "    triples_csv  = exporter.export_triples(case_id, fmt=\"csv\")\n",
        "    with open(os.path.join(out_dir, \"triples.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(triples_json)\n",
        "    with open(os.path.join(out_dir, \"triples.csv\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(triples_csv)\n",
        "\n",
        "    # 2) richer bundle (nodes, rels, evidence, runs)\n",
        "    with driver.session() as s:\n",
        "        nodes = [dict(r) for r in s.run(\"MATCH (n {case_id:$cid}) RETURN labels(n) AS labels, n AS node\", cid=case_id)]\n",
        "        rels = [dict(r) for r in s.run(\"\"\"\n",
        "            MATCH (a {case_id:$cid})-[r]->(b {case_id:$cid})\n",
        "            RETURN labels(a) AS a_labels, a AS a, type(r) AS type, r AS r, labels(b) AS b_labels, b AS b\n",
        "        \"\"\", cid=case_id)]\n",
        "        evidence = [dict(r) for r in s.run(\"\"\"\n",
        "            MATCH (d:SourceDoc {case_id:$cid})-[:HAS_CHUNK]->(k:SourceChunk)-[:HAS_SPAN]->(t:TextSpan)\n",
        "            OPTIONAL MATCH (t)-[:EVIDENCE_FOR]->(x)\n",
        "            RETURN d.doc_id AS doc_id, k.chunk_id AS chunk_id, t AS span, labels(x) AS ev_labels, x AS ev_target\n",
        "        \"\"\", cid=case_id)]\n",
        "        runs = [dict(r) for r in s.run(\"MATCH (r:ExtractionRun {case_id:$cid}) RETURN r\", cid=case_id)]\n",
        "\n",
        "    bundle = {\n",
        "        \"case_id\": case_id,\n",
        "        \"exported_at\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"nodes\": nodes,\n",
        "        \"relationships\": rels,\n",
        "        \"evidence\": evidence,\n",
        "        \"runs\": runs,\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(out_dir, \"case_bundle.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(bundle, f, indent=2, default=str)\n",
        "\n",
        "    # 3) zip it\n",
        "    zip_path = f\"{out_dir}_{case_id[:18]}.zip\"\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "        for root, _, files_ in os.walk(out_dir):\n",
        "            for fn in files_:\n",
        "                p = os.path.join(root, fn)\n",
        "                z.write(p, arcname=os.path.relpath(p, out_dir))\n",
        "\n",
        "    print(f\"‚úÖ Bundle created: {zip_path}\")\n",
        "    return zip_path\n",
        "\n",
        "def download_case_bundle(zip_path: str) -> None:\n",
        "    files.download(zip_path)\n",
        "\n",
        "print(\"‚úÖ Bundle export ready:\")\n",
        "print(\"   zip_path = export_case_bundle(case_id)\")\n",
        "print(\"   download_case_bundle(zip_path)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUq1cjW4ktpD"
      },
      "source": [
        "---\n",
        "# üîÅ BLOCK 11: Transfer Neo4j ‚Üí FalkorDB (per-case)\n",
        "**Goal:** Move a case graph into FalkorDB (RedisGraph-compatible), case-scoped and incremental.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgQf5NY3b3w1"
      },
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# BLOCK 11: NEO4J ‚Üí FALKORDB TRANSFER (CASE-SCOPED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "from falkordb import FalkorDB\n",
        "from redis import ConnectionPool\n",
        "\n",
        "def connect_falkordb(host: str, port: int = 6379, username: str = None, password: str = None, ssl: bool = False):\n",
        "    \"\"\"\n",
        "    Best-effort connector:\n",
        "    - Local/self-hosted: host/port\n",
        "    - Cloud/auth: supply username/password and set ssl=True if required\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if username or password or ssl:\n",
        "            pool = ConnectionPool(host=host, port=port, username=username, password=password, ssl=ssl, decode_responses=True)\n",
        "            return FalkorDB(connection_pool=pool)\n",
        "        return FalkorDB(host=host, port=port)\n",
        "    except TypeError:\n",
        "        # Fallback for older client versions\n",
        "        return FalkorDB(host=host, port=port)\n",
        "\n",
        "def _neo4j_fetch_case(driver, case_id: str):\n",
        "    with driver.session() as s:\n",
        "        nodes = [dict(r) for r in s.run(\"\"\"\n",
        "            MATCH (n {case_id:$cid})\n",
        "            RETURN id(n) AS neo_id, labels(n) AS labels, properties(n) AS props\n",
        "        \"\"\", cid=case_id)]\n",
        "        rels = [dict(r) for r in s.run(\"\"\"\n",
        "            MATCH (a {case_id:$cid})-[r]->(b {case_id:$cid})\n",
        "            RETURN id(a) AS src, id(b) AS dst, type(r) AS type, properties(r) AS props\n",
        "        \"\"\", cid=case_id)]\n",
        "    return nodes, rels\n",
        "\n",
        "def transfer_case_to_falkordb(case_id: str, falkor_host: str, falkor_port: int, graph_name: str,\n",
        "                              username: str = None, password: str = None, ssl: bool = False,\n",
        "                              batch_size: int = 500) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Transfers nodes + relationships for a single case_id.\n",
        "    Nodes grouped by primary label; each stored with _neo4j_id for re-linking edges.\n",
        "    \"\"\"\n",
        "    db = connect_falkordb(falkor_host, falkor_port, username=username, password=password, ssl=ssl)\n",
        "    g = db.select_graph(graph_name)\n",
        "\n",
        "    nodes, rels = _neo4j_fetch_case(driver, case_id)\n",
        "    if not nodes:\n",
        "        print(\"‚ö†Ô∏è No nodes found for this case_id.\")\n",
        "        return {\"nodes\": 0, \"rels\": 0}\n",
        "\n",
        "    # 1) Create nodes grouped by label\n",
        "    by_label = defaultdict(list)\n",
        "    for n in nodes:\n",
        "        labels = n.get(\"labels\") or [\"Node\"]\n",
        "        lbl = labels[0] if labels else \"Node\"\n",
        "        props = n.get(\"props\") or {}\n",
        "        props[\"_neo4j_id\"] = int(n[\"neo_id\"])\n",
        "        props[\"_neo_labels\"] = labels\n",
        "        by_label[lbl].append({\"props\": props})\n",
        "\n",
        "    node_created = 0\n",
        "    for lbl, rows in by_label.items():\n",
        "        for i in range(0, len(rows), batch_size):\n",
        "            batch = rows[i:i+batch_size]\n",
        "            q = f\"UNWIND $rows AS row CREATE (n:`{lbl}`) SET n = row.props\"\n",
        "            g.query(q, {\"rows\": batch})\n",
        "            node_created += len(batch)\n",
        "\n",
        "    # 2) Create relationships grouped by type\n",
        "    by_type = defaultdict(list)\n",
        "    for r in rels:\n",
        "        props = r.get(\"props\") or {}\n",
        "        props[\"_neo_rel_type\"] = r[\"type\"]\n",
        "        by_type[r[\"type\"]].append({\"src\": int(r[\"src\"]), \"dst\": int(r[\"dst\"]), \"props\": props})\n",
        "\n",
        "    rel_created = 0\n",
        "    for rt, rows in by_type.items():\n",
        "        for i in range(0, len(rows), batch_size):\n",
        "            batch = rows[i:i+batch_size]\n",
        "            q = f\"\"\"\n",
        "            UNWIND $rows AS row\n",
        "            MATCH (a) WHERE a._neo4j_id = row.src\n",
        "            MATCH (b) WHERE b._neo4j_id = row.dst\n",
        "            CREATE (a)-[r:`{rt}`]->(b)\n",
        "            SET r = row.props\n",
        "            \"\"\"\n",
        "            g.query(q, {\"rows\": batch})\n",
        "            rel_created += len(batch)\n",
        "\n",
        "    print(f\"‚úÖ Falkor transfer complete: nodes={node_created} rels={rel_created} graph={graph_name}\")\n",
        "    return {\"nodes\": node_created, \"rels\": rel_created, \"graph\": graph_name}\n",
        "\n",
        "print(\"‚úÖ FalkorDB transfer ready. Example:\")\n",
        "print(\"   transfer_case_to_falkordb(case_id, falkor_host='...', falkor_port=6379, graph_name='tacitus')\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfVrPTSkxhEm"
      },
      "source": [
        "# ‚úÖ TACITUS v7.0 Ready!\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "| Task | Block |\n",
        "|------|-------|\n",
        "| Setup & Test | Blocks 1‚Äì4A |\n",
        "| Seed Theories (GND) | Block 5 |\n",
        "| Ingest Case Files (CTX) | Block 6 (+6A) |\n",
        "| Dedup Suggestions (HITL) | Block 6B |\n",
        "| Evidence & QA | Blocks 6C‚Äì6D |\n",
        "| Reasoning Graph (RZN) | Block 6E |\n",
        "| Search & Analytics | Blocks 7‚Äì9 |\n",
        "| Export & Bundles | Blocks 10‚Äì10A |\n",
        "| Neo4j ‚Üí FalkorDB Transfer | Block 11 |\n",
        "\n",
        "**Design intent:** v7 adds provenance (spans), run-level metrics, optional reasoning-layer construction, and an integration path into FalkorDB.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}