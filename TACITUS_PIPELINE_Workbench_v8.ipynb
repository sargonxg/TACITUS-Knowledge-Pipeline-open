{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<a href=\"https://colab.research.google.com/github/sargonxg/TACITUS-Knowledge-Pipeline/blob/main/TACITUS_PIPELINE_Workbench_v8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# \ud83c\udfdb\ufe0f TACITUS Knowledge Pipeline Workbench v8\n\n**Purpose:** Production-grade, Colab-first knowledge pipeline workbench for TACITUS conflict & intelligence analysis."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# \ud83d\udd10 Inputs I Need\nProvide these in Colab secrets or environment variables (never hard-code secrets in the notebook):\n\n**Google Cloud / Vertex AI**\n- `GCP_PROJECT_ID`\n- `GCP_REGION`\n- `VERTEX_AUTH_METHOD` (e.g., `colab` or `service_account_json`)\n- `GEMINI_MODEL`\n- `EMBEDDING_MODEL`\n\n**FalkorDB**\n- `FALKORDB_HOST` (optional; if omitted, FalkorDBLite is used)\n- `FALKORDB_PORT` (optional)\n- `FALKORDB_USERNAME`\n- `FALKORDB_PASSWORD`\n\n**Neo4j**\n- `NEO4J_URI`\n- `NEO4J_USERNAME`\n- `NEO4J_PASSWORD`\n- `NEO4J_DATABASE`\n\n**Test Data**\n- 1\u20133 representative case files (txt/pdf/docx) OR permission to use included synthetic cases\n- Optional: any \u201cgold\u201d expected outputs or evaluation questions"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 0) Setup & Imports\n**Purpose:** Install dependencies and load libraries.\n\n**Inputs:** None\n\n**Outputs:** Installed libraries; importable modules.\n\n**Failure modes:** Missing network access or pip issues.\n\n**Cost notes:** None.\n\n**What to run next:** Run the cell once in a fresh runtime."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nimport sys\nimport json\nimport time\nimport math\nimport uuid\nimport hashlib\nimport textwrap\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Tuple\n\n# Lightweight install helper for Colab\ntry:\n    import pydantic\nexcept Exception:\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pydantic\", \"rapidfuzz\", \"ipywidgets\"])\n\ntry:\n    import rapidfuzz\nexcept Exception:\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rapidfuzz\"])\n\ntry:\n    import ipywidgets as widgets\nexcept Exception:\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ipywidgets\"])\n\ntry:\n    from neo4j import GraphDatabase\nexcept Exception:\n    GraphDatabase = None\n\ntry:\n    import pandas as pd\nexcept Exception:\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\"])\n    import pandas as pd\n\nfrom pydantic import BaseModel, Field, validator\nfrom rapidfuzz import fuzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 1) Configuration\n**Purpose:** Centralized configuration for the pipeline.\n\n**Inputs:** Env vars / Colab secrets\n\n**Outputs:** Config dictionary\n\n**Failure modes:** Missing required credentials for external services.\n\n**Cost notes:** None.\n\n**What to run next:** Run this cell before ingestion/extraction."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "CONFIG = {\n    \"gcp_project_id\": os.getenv(\"GCP_PROJECT_ID\"),\n    \"gcp_region\": os.getenv(\"GCP_REGION\"),\n    \"vertex_auth_method\": os.getenv(\"VERTEX_AUTH_METHOD\", \"colab\"),\n    \"gemini_model\": os.getenv(\"GEMINI_MODEL\", \"gemini-1.5-pro\"),\n    \"embedding_model\": os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-004\"),\n    \"falkordb_host\": os.getenv(\"FALKORDB_HOST\"),\n    \"falkordb_port\": os.getenv(\"FALKORDB_PORT\"),\n    \"falkordb_username\": os.getenv(\"FALKORDB_USERNAME\"),\n    \"falkordb_password\": os.getenv(\"FALKORDB_PASSWORD\"),\n    \"neo4j_uri\": os.getenv(\"NEO4J_URI\"),\n    \"neo4j_username\": os.getenv(\"NEO4J_USERNAME\"),\n    \"neo4j_password\": os.getenv(\"NEO4J_PASSWORD\"),\n    \"neo4j_database\": os.getenv(\"NEO4J_DATABASE\", \"neo4j\"),\n}\n\nCONFIG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 2) Deterministic IDs, Cleaning, and Chunking\n**Purpose:** Stable IDs, text cleaning, and chunking.\n\n**Inputs:** Raw text.\n\n**Outputs:** Cleaned text, chunks, audit info.\n\n**Failure modes:** None.\n\n**Cost notes:** None.\n\n**What to run next:** Run this cell before ingestion."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def mk_id(*parts: str) -> str:\n    seed = \"::\".join(parts)\n    return hashlib.sha256(seed.encode(\"utf-8\")).hexdigest()[:24]\n\n\ndef clean_text(text: str) -> Tuple[str, Dict[str, int]]:\n    audit = {\"removed_nulls\": text.count(\"\u0000\"), \"removed_control\": 0}\n    cleaned = \"\".join(ch for ch in text if ch.isprintable() or ch in \"\\n\\t\")\n    audit[\"removed_control\"] = len(text) - len(cleaned)\n    return cleaned, audit\n\n\ndef chunk_text(text: str, chunk_size: int = 1200, overlap: int = 150) -> List[str]:\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = min(len(text), start + chunk_size)\n        chunks.append(text[start:end])\n        start = end - overlap\n        if start < 0:\n            start = 0\n        if start >= len(text):\n            break\n    return chunks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 3) Evidence + Extraction Schemas\n**Purpose:** Schema-constrained extraction with validation.\n\n**Inputs:** Model responses.\n\n**Outputs:** Validated objects.\n\n**Failure modes:** Validation errors.\n\n**Cost notes:** LLM calls (if enabled).\n\n**What to run next:** Run this cell before extraction."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class EvidenceCandidate(BaseModel):\n    quote: str = Field(..., min_length=1)\n\nclass ExtractedClaim(BaseModel):\n    claim_id: str\n    claim_type: str\n    actor: str\n    target: Optional[str] = None\n    statement: str\n    confidence: float = Field(..., ge=0.0, le=1.0)\n    evidence: List[EvidenceCandidate]\n\n    @validator(\"claim_type\")\n    def claim_type_not_empty(cls, v):\n        if not v.strip():\n            raise ValueError(\"claim_type must not be empty\")\n        return v\n\nclass ExtractionResult(BaseModel):\n    claims: List[ExtractedClaim] = []",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 4) In-Memory Graph Store (Fallback)\n**Purpose:** Provide a runnable fallback when Neo4j/FalkorDB are unavailable.\n\n**Inputs:** Node/edge data.\n\n**Outputs:** In-memory graph.\n\n**Failure modes:** None.\n\n**Cost notes:** None.\n\n**What to run next:** Run this cell before ingestion."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass InMemoryGraph:\n    nodes: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n    edges: List[Dict[str, Any]] = field(default_factory=list)\n\n    def upsert_node(self, node_id: str, labels: List[str], properties: Dict[str, Any]):\n        existing = self.nodes.get(node_id, {\"labels\": set(), \"properties\": {}})\n        existing[\"labels\"].update(labels)\n        existing[\"properties\"].update(properties)\n        self.nodes[node_id] = existing\n\n    def add_edge(self, src: str, rel: str, dst: str, properties: Optional[Dict[str, Any]] = None):\n        self.edges.append({\"src\": src, \"rel\": rel, \"dst\": dst, \"properties\": properties or {}})\n\n    def counts_by_label(self):\n        counts = {}\n        for node in self.nodes.values():\n            for label in node[\"labels\"]:\n                counts[label] = counts.get(label, 0) + 1\n        return counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 5) Neo4j Connection & Vector Index Helpers\n**Purpose:** Connect to Neo4j and prepare vector indexes.\n\n**Inputs:** Neo4j credentials.\n\n**Outputs:** Driver object and helper functions.\n\n**Failure modes:** Missing creds or network access.\n\n**Cost notes:** None.\n\n**What to run next:** Run this cell before Neo4j ingestion."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def get_neo4j_driver(config: Dict[str, Any]):\n    if not config.get(\"neo4j_uri\") or not GraphDatabase:\n        return None\n    return GraphDatabase.driver(\n        config[\"neo4j_uri\"],\n        auth=(config.get(\"neo4j_username\"), config.get(\"neo4j_password\")),\n    )\n\n\ndef ensure_vector_index(driver, index_name: str, label: str, property_name: str, dims: int = 768):\n    if not driver:\n        return False\n    cypher = f\"\"\"\n    CREATE VECTOR INDEX {index_name} IF NOT EXISTS\n    FOR (n:{label})\n    ON (n.{property_name})\n    OPTIONS {{ indexConfig: {{`vector.dimensions`: {dims}, `vector.similarity_function`: 'cosine'}} }}\n    \"\"\"\n    with driver.session(database=CONFIG.get(\"neo4j_database\")) as session:\n        try:\n            session.run(cypher)\n            return True\n        except Exception as e:\n            print(\"Vector index creation failed, falling back:\", e)\n            return False",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 6) LLM Extraction (Structured Output + Retry + Repair)\n**Purpose:** Robust extraction with retries and schema validation.\n\n**Inputs:** Chunk text.\n\n**Outputs:** ExtractionResult.\n\n**Failure modes:** API errors, validation errors.\n\n**Cost notes:** Vertex/Gemini calls per chunk.\n\n**What to run next:** Run this cell before extraction."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def _mock_extract(chunk: str) -> ExtractionResult:\n    # Minimal mock extractor to allow notebook to run without external LLMs.\n    claim = ExtractedClaim(\n        claim_id=mk_id(\"claim\", chunk[:40]),\n        claim_type=\"mock\",\n        actor=\"Unknown\",\n        target=None,\n        statement=chunk[:200].strip(),\n        confidence=0.1,\n        evidence=[EvidenceCandidate(quote=chunk[:80].strip())],\n    )\n    return ExtractionResult(claims=[claim])\n\n\ndef extract_with_retries(chunk: str, max_retries: int = 3) -> ExtractionResult:\n    # Placeholder: implement Vertex structured output if credentials are present.\n    for attempt in range(max_retries):\n        try:\n            return _mock_extract(chunk)\n        except Exception:\n            wait = 2 ** attempt\n            time.sleep(wait)\n    return ExtractionResult(claims=[])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 7) Evidence Matching\n**Purpose:** Link extracted evidence quotes to text spans.\n\n**Inputs:** Extracted objects and source text.\n\n**Outputs:** Span matches with scores.\n\n**Failure modes:** None.\n\n**Cost notes:** None.\n\n**What to run next:** Run this cell before writing evidence links."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def find_best_span(text: str, quote: str) -> Tuple[int, int, float]:\n    if not quote:\n        return 0, 0, 0.0\n    best_score = 0.0\n    best_span = (0, 0)\n    for i in range(0, max(len(text) - len(quote), 0), max(1, len(quote)//5)):\n        window = text[i:i+len(quote)]\n        score = fuzz.partial_ratio(quote, window)\n        if score > best_score:\n            best_score = score\n            best_span = (i, i+len(window))\n    return best_span[0], best_span[1], best_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 8) Unified Ingestion Pipeline\n**Purpose:** Single canonical entrypoint for ingestion.\n\n**Inputs:** Files, case name, options.\n\n**Outputs:** Case graph + evidence.\n\n**Failure modes:** Missing files, DB connection failures.\n\n**Cost notes:** LLM calls + embedding calls.\n\n**What to run next:** Run this cell before using the Workbench UI."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def ingest_case(\n    filename: str,\n    file_bytes: bytes,\n    case_name: str,\n    mode: str = \"dry_run\",\n    with_evidence: bool = True,\n    chunk_size: int = 1200,\n    overlap: int = 150,\n):\n    text = file_bytes.decode(\"utf-8\", errors=\"ignore\")\n    cleaned, audit = clean_text(text)\n    chunks = chunk_text(cleaned, chunk_size=chunk_size, overlap=overlap)\n\n    run_id = mk_id(\"run\", case_name, str(time.time()))\n    case_id = mk_id(\"case\", case_name)\n\n    mem_graph = InMemoryGraph()\n    mem_graph.upsert_node(case_id, [\"Case\", \"CTX\"], {\"name\": case_name})\n    mem_graph.upsert_node(mk_id(\"doc\", filename), [\"SourceDoc\", \"EVD\"], {\n        \"filename\": filename,\n        \"case_id\": case_id,\n        \"cleaning_audit\": audit,\n    })\n\n    for idx, chunk in enumerate(chunks):\n        chunk_id = mk_id(\"chunk\", case_id, str(idx))\n        mem_graph.upsert_node(chunk_id, [\"Chunk\", \"EVD\"], {\"index\": idx, \"text\": chunk})\n        mem_graph.add_edge(case_id, \"HAS_CHUNK\", chunk_id)\n\n        extraction = extract_with_retries(chunk)\n        for claim in extraction.claims:\n            claim_node_id = claim.claim_id\n            mem_graph.upsert_node(claim_node_id, [\"Claim\", \"CTX\"], claim.dict())\n            mem_graph.add_edge(case_id, \"HAS_CLAIM\", claim_node_id)\n            if with_evidence:\n                for ev in claim.evidence:\n                    start, end, score = find_best_span(chunk, ev.quote)\n                    span_id = mk_id(\"span\", chunk_id, str(start), str(end))\n                    mem_graph.upsert_node(span_id, [\"Span\", \"EVD\"], {\n                        \"start\": start,\n                        \"end\": end,\n                        \"score\": score,\n                        \"quote\": ev.quote,\n                    })\n                    mem_graph.add_edge(claim_node_id, \"SUPPORTED_BY\", span_id)\n\n    return {\n        \"case_id\": case_id,\n        \"run_id\": run_id,\n        \"graph\": mem_graph,\n        \"chunks\": len(chunks),\n    }",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 9) Workbench UI\n**Purpose:** Interactive controls for ingestion, replication, QA, and benchmarks.\n\n**Inputs:** None.\n\n**Outputs:** Widgets and live stats.\n\n**Failure modes:** Widgets not available.\n\n**Cost notes:** None.\n\n**What to run next:** Run this cell to launch the workbench."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "case_name_input = widgets.Text(description=\"Case Name\", value=\"Demo Case\")\nfile_upload = widgets.FileUpload(accept=\".txt\", multiple=False)\nmode_select = widgets.Dropdown(options=[\"dry_run\", \"neo4j_only\", \"neo4j_then_falkor\"], value=\"dry_run\")\nwith_evidence_checkbox = widgets.Checkbox(value=True, description=\"With evidence\")\n\nrun_button = widgets.Button(description=\"Ingest new case\")\noutput = widgets.Output()\n\nstate = {}\n\n\ndef on_run_clicked(_):\n    with output:\n        output.clear_output()\n        if not file_upload.value:\n            print(\"Upload a .txt file to ingest.\")\n            return\n        uploaded = next(iter(file_upload.value.values()))\n        result = ingest_case(\n            filename=uploaded[\"metadata\"][\"name\"],\n            file_bytes=uploaded[\"content\"],\n            case_name=case_name_input.value,\n            mode=mode_select.value,\n            with_evidence=with_evidence_checkbox.value,\n        )\n        state.update(result)\n        print(\"Ingested case:\", result[\"case_id\"], \"chunks:\", result[\"chunks\"])\n        print(\"Node counts:\", result[\"graph\"].counts_by_label())\n\nrun_button.on_click(on_run_clicked)\n\nwidgets.VBox([\n    case_name_input,\n    file_upload,\n    mode_select,\n    with_evidence_checkbox,\n    run_button,\n    output,\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 10) Testing & Evaluation\n**Purpose:** Evidence coverage and quality checks.\n\n**Inputs:** In-memory graph results.\n\n**Outputs:** Metrics.\n\n**Failure modes:** None.\n\n**Cost notes:** None.\n\n**What to run next:** Run after an ingestion run."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_evidence_coverage(graph: InMemoryGraph) -> float:\n    claim_ids = [node_id for node_id, node in graph.nodes.items() if \"Claim\" in node[\"labels\"]]\n    supported = 0\n    for claim_id in claim_ids:\n        if any(edge for edge in graph.edges if edge[\"src\"] == claim_id and edge[\"rel\"] == \"SUPPORTED_BY\"):\n            supported += 1\n    return supported / max(len(claim_ids), 1)\n\nif state.get(\"graph\"):\n    coverage = compute_evidence_coverage(state[\"graph\"])\n    print(\"Evidence coverage:\", round(coverage * 100, 2), \"%\")\nelse:\n    print(\"Run ingestion first.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 11) Benchmarking (Mock)\n**Purpose:** Compare Neo4j vs FalkorDB (mocked in this fallback path).\n\n**Inputs:** Ingestion stats.\n\n**Outputs:** Pandas comparison table.\n\n**Failure modes:** None.\n\n**Cost notes:** None.\n\n**What to run next:** Run after ingestion."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "if state.get(\"graph\"):\n    data = {\n        \"metric\": [\"nodes\", \"edges\", \"evidence_coverage\"],\n        \"neo4j\": [len(state[\"graph\"].nodes), len(state[\"graph\"].edges), compute_evidence_coverage(state[\"graph\"])],\n        \"falkordb\": [len(state[\"graph\"].nodes), len(state[\"graph\"].edges), compute_evidence_coverage(state[\"graph\"])],\n    }\n    df = pd.DataFrame(data)\n    display(df)\nelse:\n    print(\"Run ingestion first.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 12) Run Summary\n**Purpose:** Summarize run status.\n\n**Inputs:** Current state.\n\n**Outputs:** Summary block.\n\n**Failure modes:** None.\n\n**Cost notes:** None.\n\n**What to run next:** End of notebook."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "if state.get(\"graph\"):\n    print(\"case_id:\", state[\"case_id\"])\n    print(\"run_id:\", state[\"run_id\"])\n    print(\"nodes:\", len(state[\"graph\"].nodes))\n    print(\"edges:\", len(state[\"graph\"].edges))\n    print(\"evidence_coverage:\", round(compute_evidence_coverage(state[\"graph\"]) * 100, 2), \"%\")\nelse:\n    print(\"No run yet. Use the Workbench UI to ingest a case.\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}